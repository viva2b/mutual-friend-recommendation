package concurrency

import (
	"context"
	"fmt"
	"sync"
	"sync/atomic"
	"testing"
	"time"

	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"

	"mutual-friend/internal/domain"
	"mutual-friend/pkg/cache"
	"mutual-friend/pkg/config"
	"mutual-friend/pkg/redis"
)

// ConcurrencyTester ë™ì‹œì„± í…ŒìŠ¤íŠ¸ êµ¬ì¡°ì²´
type ConcurrencyTester struct {
	cacheService cache.Cache
	ctx          context.Context
	metrics      *ConcurrencyMetrics
}

// ConcurrencyMetrics ë™ì‹œì„± ê´€ë ¨ ë©”íŠ¸ë¦­
type ConcurrencyMetrics struct {
	mu                    sync.RWMutex
	
	// ê²½í•© ê´€ë ¨ ë©”íŠ¸ë¦­
	RaceConditions        int64
	DataInconsistencies   int64
	CacheConflicts        int64
	DeadlockAttempts      int64
	
	// ì„±ëŠ¥ ì˜í–¥ ë©”íŠ¸ë¦­
	ContentionDelay       time.Duration
	LockWaitTime          time.Duration
	RetryAttempts         int64
	
	// ë¬´ê²°ì„± ë©”íŠ¸ë¦­
	DataCorruptions       int64
	LostUpdates           int64
	DirtyReads            int64
	PhantomReads          int64
	
	// ë¦¬ì†ŒìŠ¤ ê²½í•© ë©”íŠ¸ë¦­
	MemoryContention      int64
	ConnectionContention  int64
	ThreadContention      int64
}

func (cm *ConcurrencyMetrics) IncrementRaceCondition() {
	atomic.AddInt64(&cm.RaceConditions, 1)
}

func (cm *ConcurrencyMetrics) IncrementDataInconsistency() {
	atomic.AddInt64(&cm.DataInconsistencies, 1)
}

func (cm *ConcurrencyMetrics) IncrementCacheConflict() {
	atomic.AddInt64(&cm.CacheConflicts, 1)
}

func (cm *ConcurrencyMetrics) IncrementDeadlockAttempt() {
	atomic.AddInt64(&cm.DeadlockAttempts, 1)
}

func (cm *ConcurrencyMetrics) AddContentionDelay(duration time.Duration) {
	cm.mu.Lock()
	defer cm.mu.Unlock()
	cm.ContentionDelay += duration
}

func (cm *ConcurrencyMetrics) IncrementRetryAttempt() {
	atomic.AddInt64(&cm.RetryAttempts, 1)
}

func (cm *ConcurrencyMetrics) IncrementDataCorruption() {
	atomic.AddInt64(&cm.DataCorruptions, 1)
}

func (cm *ConcurrencyMetrics) IncrementLostUpdate() {
	atomic.AddInt64(&cm.LostUpdates, 1)
}

// NewConcurrencyTester ë™ì‹œì„± í…ŒìŠ¤í„° ìƒì„±
func NewConcurrencyTester(t *testing.T) *ConcurrencyTester {
	// ì„¤ì • ë¡œë“œ
	cfg, err := config.LoadConfig("../../configs/config.yaml")
	require.NoError(t, err)
	
	// Redis í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
	redisClient, err := redis.NewClient(&redis.Config{
		Address:      cfg.Redis.Address,
		Password:     cfg.Redis.Password,
		Database:     cfg.Redis.Database,
		PoolSize:     cfg.Redis.PoolSize,
		MinIdleConns: cfg.Redis.MinIdleConns,
		MaxRetries:   cfg.Redis.MaxRetries,
		DialTimeout:  time.Duration(cfg.Redis.DialTimeout) * time.Second,
		ReadTimeout:  time.Duration(cfg.Redis.ReadTimeout) * time.Second,
		WriteTimeout: time.Duration(cfg.Redis.WriteTimeout) * time.Second,
	})
	require.NoError(t, err)
	
	// ìºì‹œ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
	cacheService, err := cache.NewService(redisClient)
	require.NoError(t, err)
	
	return &ConcurrencyTester{
		cacheService: cacheService,
		ctx:          context.Background(),
		metrics:      &ConcurrencyMetrics{},
	}
}

// TestRaceConditions ë ˆì´ìŠ¤ ì»¨ë””ì…˜ í…ŒìŠ¤íŠ¸
func TestRaceConditions(t *testing.T) {
	tester := NewConcurrencyTester(t)
	defer tester.cacheService.Close()
	
	t.Run("Write-Write Race Condition", func(t *testing.T) {
		tester.testWriteWriteRaceCondition(t)
	})
	
	t.Run("Read-Write Race Condition", func(t *testing.T) {
		tester.testReadWriteRaceCondition(t)
	})
	
	t.Run("Cache Invalidation Race", func(t *testing.T) {
		tester.testCacheInvalidationRace(t)
	})
	
	t.Run("Counter Race Condition", func(t *testing.T) {
		tester.testCounterRaceCondition(t)
	})
}

// testWriteWriteRaceCondition Write-Write ë ˆì´ìŠ¤ ì»¨ë””ì…˜ í…ŒìŠ¤íŠ¸
func (ct *ConcurrencyTester) testWriteWriteRaceCondition(t *testing.T) {
	t.Log("ğŸ” Testing Write-Write Race Conditions")
	
	const (
		numWriters = 100
		testKey    = "race_write_test"
	)
	
	var wg sync.WaitGroup
	var successfulWrites int64
	var failedWrites int64
	
	// ì´ˆê¸° ê°’ ì„¤ì •
	err := ct.cacheService.Set(ct.ctx, testKey, "initial_value", time.Hour)
	require.NoError(t, err)
	
	start := time.Now()
	
	// ë™ì‹œì— ê°™ì€ í‚¤ì— ì“°ê¸°
	for i := 0; i < numWriters; i++ {
		wg.Add(1)
		go func(writerID int) {
			defer wg.Done()
			
			value := fmt.Sprintf("writer_%d_value", writerID)
			
			writeStart := time.Now()
			err := ct.cacheService.Set(ct.ctx, testKey, value, time.Hour)
			writeTime := time.Since(writeStart)
			
			if writeTime > 10*time.Millisecond {
				ct.metrics.AddContentionDelay(writeTime)
			}
			
			if err != nil {
				atomic.AddInt64(&failedWrites, 1)
				ct.metrics.IncrementRaceCondition()
			} else {
				atomic.AddInt64(&successfulWrites, 1)
			}
		}(i)
	}
	
	wg.Wait()
	duration := time.Since(start)
	
	// ìµœì¢… ê°’ í™•ì¸
	finalValue, err := ct.cacheService.Get(ct.ctx, testKey)
	require.NoError(t, err)
	
	t.Logf("=== Write-Write Race Condition Results ===")
	t.Logf("Total Writers: %d", numWriters)
	t.Logf("Successful Writes: %d", successfulWrites)
	t.Logf("Failed Writes: %d", failedWrites)
	t.Logf("Final Value: %s", finalValue)
	t.Logf("Total Duration: %v", duration)
	t.Logf("Race Conditions Detected: %d", ct.metrics.RaceConditions)
	
	// ê²€ì¦
	assert.Equal(t, numWriters, int(successfulWrites+failedWrites), "Total operations mismatch")
	assert.NotEqual(t, "initial_value", finalValue, "Value should have been updated")
	
	// ê²½ê³  ì„ê³„ê°’
	if ct.metrics.RaceConditions > int64(numWriters/10) {
		t.Logf("âš ï¸  High race condition rate: %d/%d", ct.metrics.RaceConditions, numWriters)
	}
}

// testReadWriteRaceCondition Read-Write ë ˆì´ìŠ¤ ì»¨ë””ì…˜ í…ŒìŠ¤íŠ¸
func (ct *ConcurrencyTester) testReadWriteRaceCondition(t *testing.T) {
	t.Log("ğŸ” Testing Read-Write Race Conditions")
	
	const (
		numReaders = 50
		numWriters = 10
		testKey    = "race_rw_test"
	)
	
	var wg sync.WaitGroup
	var readValues []string
	var writeValues []string
	var mu sync.Mutex
	
	// ì´ˆê¸° ê°’ ì„¤ì •
	initialValue := "initial_rw_value"
	err := ct.cacheService.Set(ct.ctx, testKey, initialValue, time.Hour)
	require.NoError(t, err)
	
	start := time.Now()
	
	// ì½ê¸° ê³ ë£¨í‹´ë“¤
	for i := 0; i < numReaders; i++ {
		wg.Add(1)
		go func(readerID int) {
			defer wg.Done()
			
			// ëœë¤ ì§€ì—°ìœ¼ë¡œ ì½ê¸° íƒ€ì´ë° ë¶„ì‚°
			time.Sleep(time.Duration(readerID%10) * time.Millisecond)
			
			value, err := ct.cacheService.Get(ct.ctx, testKey)
			if err != nil {
				ct.metrics.IncrementRaceCondition()
				return
			}
			
			mu.Lock()
			readValues = append(readValues, value)
			mu.Unlock()
		}(i)
	}
	
	// ì“°ê¸° ê³ ë£¨í‹´ë“¤
	for i := 0; i < numWriters; i++ {
		wg.Add(1)
		go func(writerID int) {
			defer wg.Done()
			
			// ëœë¤ ì§€ì—°ìœ¼ë¡œ ì“°ê¸° íƒ€ì´ë° ë¶„ì‚°
			time.Sleep(time.Duration(writerID%5) * time.Millisecond)
			
			value := fmt.Sprintf("writer_%d_updated", writerID)
			err := ct.cacheService.Set(ct.ctx, testKey, value, time.Hour)
			if err != nil {
				ct.metrics.IncrementRaceCondition()
				return
			}
			
			mu.Lock()
			writeValues = append(writeValues, value)
			mu.Unlock()
		}(i)
	}
	
	wg.Wait()
	duration := time.Since(start)
	
	// ìµœì¢… ê°’ í™•ì¸
	finalValue, err := ct.cacheService.Get(ct.ctx, testKey)
	require.NoError(t, err)
	
	t.Logf("=== Read-Write Race Condition Results ===")
	t.Logf("Readers: %d, Writers: %d", numReaders, numWriters)
	t.Logf("Read Values Collected: %d", len(readValues))
	t.Logf("Write Values Collected: %d", len(writeValues))
	t.Logf("Final Value: %s", finalValue)
	t.Logf("Duration: %v", duration)
	
	// ë°ì´í„° ì¼ê´€ì„± ê²€ì¦
	ct.analyzeReadWriteConsistency(t, readValues, writeValues, initialValue, finalValue)
}

// analyzeReadWriteConsistency ì½ê¸°-ì“°ê¸° ì¼ê´€ì„± ë¶„ì„
func (ct *ConcurrencyTester) analyzeReadWriteConsistency(t *testing.T, readValues, writeValues []string, initialValue, finalValue string) {
	// ì½ê¸° ê°’ë“¤ ë¶„ì„
	readValueCounts := make(map[string]int)
	for _, value := range readValues {
		readValueCounts[value]++
	}
	
	// ì“°ê¸° ê°’ë“¤ ë¶„ì„
	writeValueCounts := make(map[string]int)
	for _, value := range writeValues {
		writeValueCounts[value]++
	}
	
	t.Logf("=== Read-Write Consistency Analysis ===")
	t.Logf("Unique Read Values: %d", len(readValueCounts))
	t.Logf("Unique Write Values: %d", len(writeValueCounts))
	
	// ë¹„ì •ìƒì ì¸ ì½ê¸° ê°ì§€
	for value, count := range readValueCounts {
		if value != initialValue && writeValueCounts[value] == 0 {
			ct.metrics.IncrementDataInconsistency()
			t.Logf("ğŸ”´ Phantom read detected: %s (count: %d)", value, count)
		}
	}
	
	// ë”í‹° ì½ê¸° ê°ì§€
	if readValueCounts[initialValue] > 0 && len(writeValues) > 0 {
		t.Logf("ğŸŸ¡ Potential dirty read: initial value read %d times after writes", readValueCounts[initialValue])
	}
	
	t.Logf("Data Inconsistencies: %d", ct.metrics.DataInconsistencies)
}

// testCacheInvalidationRace ìºì‹œ ë¬´íš¨í™” ê²½í•© í…ŒìŠ¤íŠ¸
func (ct *ConcurrencyTester) testCacheInvalidationRace(t *testing.T) {
	t.Log("ğŸ” Testing Cache Invalidation Race Conditions")
	
	const (
		numInvalidators = 20
		numSetters      = 30
		testKeyPrefix   = "invalidation_race"
	)
	
	var wg sync.WaitGroup
	var invalidationConflicts int64
	var setterConflicts int64
	
	// ì´ˆê¸° ë°ì´í„° ì„¤ì •
	for i := 0; i < 10; i++ {
		key := fmt.Sprintf("%s_%d", testKeyPrefix, i)
		value := fmt.Sprintf("initial_value_%d", i)
		err := ct.cacheService.Set(ct.ctx, key, value, time.Hour)
		require.NoError(t, err)
	}
	
	start := time.Now()
	
	// ë¬´íš¨í™” ê³ ë£¨í‹´ë“¤
	for i := 0; i < numInvalidators; i++ {
		wg.Add(1)
		go func(invalidatorID int) {
			defer wg.Done()
			
			for j := 0; j < 10; j++ {
				key := fmt.Sprintf("%s_%d", testKeyPrefix, j%10)
				
				invalidateStart := time.Now()
				_, err := ct.cacheService.Delete(ct.ctx, key)
				invalidateTime := time.Since(invalidateStart)
				
				if invalidateTime > 5*time.Millisecond {
					ct.metrics.AddContentionDelay(invalidateTime)
				}
				
				if err != nil {
					atomic.AddInt64(&invalidationConflicts, 1)
					ct.metrics.IncrementCacheConflict()
				}
				
				time.Sleep(time.Millisecond) // ì•½ê°„ì˜ ì§€ì—°
			}
		}(i)
	}
	
	// ì„¤ì • ê³ ë£¨í‹´ë“¤
	for i := 0; i < numSetters; i++ {
		wg.Add(1)
		go func(setterID int) {
			defer wg.Done()
			
			for j := 0; j < 10; j++ {
				key := fmt.Sprintf("%s_%d", testKeyPrefix, j%10)
				value := fmt.Sprintf("setter_%d_value_%d", setterID, j)
				
				setStart := time.Now()
				err := ct.cacheService.Set(ct.ctx, key, value, time.Hour)
				setTime := time.Since(setStart)
				
				if setTime > 5*time.Millisecond {
					ct.metrics.AddContentionDelay(setTime)
				}
				
				if err != nil {
					atomic.AddInt64(&setterConflicts, 1)
					ct.metrics.IncrementCacheConflict()
				}
				
				time.Sleep(time.Millisecond) // ì•½ê°„ì˜ ì§€ì—°
			}
		}(i)
	}
	
	wg.Wait()
	duration := time.Since(start)
	
	// ìµœì¢… ìƒíƒœ í™•ì¸
	finalStates := make(map[string]string)
	for i := 0; i < 10; i++ {
		key := fmt.Sprintf("%s_%d", testKeyPrefix, i)
		value, err := ct.cacheService.Get(ct.ctx, key)
		if err != nil {
			finalStates[key] = "DELETED"
		} else {
			finalStates[key] = value
		}
	}
	
	t.Logf("=== Cache Invalidation Race Results ===")
	t.Logf("Invalidators: %d, Setters: %d", numInvalidators, numSetters)
	t.Logf("Invalidation Conflicts: %d", invalidationConflicts)
	t.Logf("Setter Conflicts: %d", setterConflicts)
	t.Logf("Total Cache Conflicts: %d", ct.metrics.CacheConflicts)
	t.Logf("Duration: %v", duration)
	
	t.Logf("=== Final Cache States ===")
	deletedCount := 0
	for key, state := range finalStates {
		t.Logf("%s: %s", key, state)
		if state == "DELETED" {
			deletedCount++
		}
	}
	t.Logf("Deleted Entries: %d/10", deletedCount)
}

// testCounterRaceCondition ì¹´ìš´í„° ë ˆì´ìŠ¤ ì»¨ë””ì…˜ í…ŒìŠ¤íŠ¸
func (ct *ConcurrencyTester) testCounterRaceCondition(t *testing.T) {
	t.Log("ğŸ” Testing Counter Race Conditions")
	
	const (
		numIncrementers = 50
		incrementCount  = 20
		testKey        = "counter_race_test"
	)
	
	// ì´ˆê¸° ì¹´ìš´í„° ê°’ ì„¤ì •
	err := ct.cacheService.Set(ct.ctx, testKey, "0", time.Hour)
	require.NoError(t, err)
	
	var wg sync.WaitGroup
	var lostUpdates int64
	
	start := time.Now()
	
	// ë™ì‹œì— ì¹´ìš´í„° ì¦ê°€
	for i := 0; i < numIncrementers; i++ {
		wg.Add(1)
		go func(incrementerID int) {
			defer wg.Done()
			
			for j := 0; j < incrementCount; j++ {
				// Read-Modify-Write íŒ¨í„´ìœ¼ë¡œ ë ˆì´ìŠ¤ ì»¨ë””ì…˜ ìœ ë„
				success := ct.incrementCounter(testKey)
				if !success {
					atomic.AddInt64(&lostUpdates, 1)
					ct.metrics.IncrementLostUpdate()
				}
			}
		}(i)
	}
	
	wg.Wait()
	duration := time.Since(start)
	
	// ìµœì¢… ì¹´ìš´í„° ê°’ í™•ì¸
	finalValueStr, err := ct.cacheService.Get(ct.ctx, testKey)
	require.NoError(t, err)
	
	expectedValue := numIncrementers * incrementCount
	
	t.Logf("=== Counter Race Condition Results ===")
	t.Logf("Incrementers: %d", numIncrementers)
	t.Logf("Increments per Incrementer: %d", incrementCount)
	t.Logf("Expected Final Value: %d", expectedValue)
	t.Logf("Actual Final Value: %s", finalValueStr)
	t.Logf("Lost Updates: %d", lostUpdates)
	t.Logf("Duration: %v", duration)
	
	// ì‹¤ì œë¡œ ì†ì‹¤ëœ ì—…ë°ì´íŠ¸ ê³„ì‚°
	var actualValue int
	fmt.Sscanf(finalValueStr, "%d", &actualValue)
	actualLostUpdates := expectedValue - actualValue
	
	t.Logf("Calculated Lost Updates: %d", actualLostUpdates)
	
	// ê²€ì¦
	assert.True(t, actualLostUpdates >= 0, "Negative lost updates detected")
	if actualLostUpdates > 0 {
		t.Logf("ğŸ”´ Lost update problem detected: %d updates lost", actualLostUpdates)
	}
}

// incrementCounter ì¹´ìš´í„° ì¦ê°€ (ë ˆì´ìŠ¤ ì»¨ë””ì…˜ì— ì·¨ì•½í•œ êµ¬í˜„)
func (ct *ConcurrencyTester) incrementCounter(key string) bool {
	// í˜„ì¬ ê°’ ì½ê¸°
	currentStr, err := ct.cacheService.Get(ct.ctx, key)
	if err != nil {
		return false
	}
	
	// ê°’ íŒŒì‹±
	var current int
	fmt.Sscanf(currentStr, "%d", &current)
	
	// ì˜ë„ì  ì§€ì—°ìœ¼ë¡œ ë ˆì´ìŠ¤ ì»¨ë””ì…˜ ìœ ë„
	time.Sleep(time.Microsecond * 10)
	
	// ì¦ê°€ëœ ê°’ ì €ì¥
	newValue := current + 1
	newValueStr := fmt.Sprintf("%d", newValue)
	
	err = ct.cacheService.Set(ct.ctx, key, newValueStr, time.Hour)
	return err == nil
}

// TestDeadlockDetection ë°ë“œë½ ê°ì§€ í…ŒìŠ¤íŠ¸
func TestDeadlockDetection(t *testing.T) {
	tester := NewConcurrencyTester(t)
	defer tester.cacheService.Close()
	
	t.Run("Resource Ordering Deadlock", func(t *testing.T) {
		tester.testResourceOrderingDeadlock(t)
	})
	
	t.Run("Circular Wait Deadlock", func(t *testing.T) {
		tester.testCircularWaitDeadlock(t)
	})
}

// testResourceOrderingDeadlock ë¦¬ì†ŒìŠ¤ ìˆœì„œ ë°ë“œë½ í…ŒìŠ¤íŠ¸
func (ct *ConcurrencyTester) testResourceOrderingDeadlock(t *testing.T) {
	t.Log("ğŸ” Testing Resource Ordering Deadlocks")
	
	const (
		numWorkers = 10
		resource1  = "resource_A"
		resource2  = "resource_B"
	)
	
	var wg sync.WaitGroup
	var deadlockAttempts int64
	var completedOperations int64
	
	// ì´ˆê¸° ë¦¬ì†ŒìŠ¤ ì„¤ì •
	err := ct.cacheService.Set(ct.ctx, resource1, "data_A", time.Hour)
	require.NoError(t, err)
	err = ct.cacheService.Set(ct.ctx, resource2, "data_B", time.Hour)
	require.NoError(t, err)
	
	start := time.Now()
	
	// ë°ë“œë½ ì‹œë‚˜ë¦¬ì˜¤ ì‹œë®¬ë ˆì´ì…˜
	for i := 0; i < numWorkers; i++ {
		wg.Add(1)
		go func(workerID int) {
			defer wg.Done()
			
			timeout := time.After(5 * time.Second)
			
			if workerID%2 == 0 {
				// Worker A: resource1 -> resource2 ìˆœì„œ
				select {
				case <-timeout:
					atomic.AddInt64(&deadlockAttempts, 1)
					ct.metrics.IncrementDeadlockAttempt()
					return
				default:
					success := ct.acquireResourcesInOrder(resource1, resource2)
					if success {
						atomic.AddInt64(&completedOperations, 1)
					}
				}
			} else {
				// Worker B: resource2 -> resource1 ìˆœì„œ (ë°ë“œë½ ìœ ë°œ ê°€ëŠ¥)
				select {
				case <-timeout:
					atomic.AddInt64(&deadlockAttempts, 1)
					ct.metrics.IncrementDeadlockAttempt()
					return
				default:
					success := ct.acquireResourcesInOrder(resource2, resource1)
					if success {
						atomic.AddInt64(&completedOperations, 1)
					}
				}
			}
		}(i)
	}
	
	wg.Wait()
	duration := time.Since(start)
	
	t.Logf("=== Resource Ordering Deadlock Results ===")
	t.Logf("Workers: %d", numWorkers)
	t.Logf("Completed Operations: %d", completedOperations)
	t.Logf("Deadlock Attempts: %d", deadlockAttempts)
	t.Logf("Duration: %v", duration)
	
	// ë°ë“œë½ ê°€ëŠ¥ì„± ë¶„ì„
	if deadlockAttempts > 0 {
		t.Logf("ğŸ”´ Potential deadlock detected: %d timeout cases", deadlockAttempts)
	}
	
	successRate := float64(completedOperations) / float64(numWorkers) * 100
	t.Logf("Success Rate: %.2f%%", successRate)
}

// acquireResourcesInOrder ë¦¬ì†ŒìŠ¤ë¥¼ ìˆœì„œëŒ€ë¡œ íšë“
func (ct *ConcurrencyTester) acquireResourcesInOrder(first, second string) bool {
	// ì²« ë²ˆì§¸ ë¦¬ì†ŒìŠ¤ íšë“ ì‹œë®¬ë ˆì´ì…˜
	_, err := ct.cacheService.Get(ct.ctx, first)
	if err != nil {
		return false
	}
	
	// ì‘ì—… ì‹œë®¬ë ˆì´ì…˜
	time.Sleep(time.Millisecond * 10)
	
	// ë‘ ë²ˆì§¸ ë¦¬ì†ŒìŠ¤ íšë“ ì‹œë®¬ë ˆì´ì…˜
	_, err = ct.cacheService.Get(ct.ctx, second)
	if err != nil {
		return false
	}
	
	// ì‘ì—… ì™„ë£Œ ì‹œë®¬ë ˆì´ì…˜
	time.Sleep(time.Millisecond * 5)
	
	return true
}

// testCircularWaitDeadlock ìˆœí™˜ ëŒ€ê¸° ë°ë“œë½ í…ŒìŠ¤íŠ¸
func (ct *ConcurrencyTester) testCircularWaitDeadlock(t *testing.T) {
	t.Log("ğŸ” Testing Circular Wait Deadlocks")
	
	const (
		numResources = 5
		numWorkers   = 10
	)
	
	var wg sync.WaitGroup
	var circularWaitDetected int64
	var completedChains int64
	
	// ë¦¬ì†ŒìŠ¤ ì²´ì¸ ì´ˆê¸°í™”
	resources := make([]string, numResources)
	for i := 0; i < numResources; i++ {
		resources[i] = fmt.Sprintf("chain_resource_%d", i)
		err := ct.cacheService.Set(ct.ctx, resources[i], fmt.Sprintf("data_%d", i), time.Hour)
		require.NoError(t, err)
	}
	
	start := time.Now()
	
	// ìˆœí™˜ ëŒ€ê¸° ì‹œë‚˜ë¦¬ì˜¤
	for i := 0; i < numWorkers; i++ {
		wg.Add(1)
		go func(workerID int) {
			defer wg.Done()
			
			timeout := time.After(3 * time.Second)
			
			select {
			case <-timeout:
				atomic.AddInt64(&circularWaitDetected, 1)
				return
			default:
				// ë¦¬ì†ŒìŠ¤ ì²´ì¸ ìˆœì°¨ íšë“
				success := ct.acquireResourceChain(resources, workerID)
				if success {
					atomic.AddInt64(&completedChains, 1)
				}
			}
		}(i)
	}
	
	wg.Wait()
	duration := time.Since(start)
	
	t.Logf("=== Circular Wait Deadlock Results ===")
	t.Logf("Resources: %d, Workers: %d", numResources, numWorkers)
	t.Logf("Completed Chains: %d", completedChains)
	t.Logf("Circular Wait Detected: %d", circularWaitDetected)
	t.Logf("Duration: %v", duration)
	
	if circularWaitDetected > 0 {
		t.Logf("ğŸ”´ Circular wait deadlock detected: %d cases", circularWaitDetected)
	}
}

// acquireResourceChain ë¦¬ì†ŒìŠ¤ ì²´ì¸ íšë“
func (ct *ConcurrencyTester) acquireResourceChain(resources []string, workerID int) bool {
	// ì›Œì»¤ë³„ë¡œ ë‹¤ë¥¸ ì‹œì‘ì ìœ¼ë¡œ ìˆœí™˜ íŒ¨í„´ ìƒì„±
	startIndex := workerID % len(resources)
	
	for i := 0; i < len(resources); i++ {
		resourceIndex := (startIndex + i) % len(resources)
		resource := resources[resourceIndex]
		
		_, err := ct.cacheService.Get(ct.ctx, resource)
		if err != nil {
			return false
		}
		
		// ë¦¬ì†ŒìŠ¤ ë³´ìœ  ì‹œë®¬ë ˆì´ì…˜
		time.Sleep(time.Millisecond * time.Duration(10+i))
	}
	
	return true
}

// TestMemoryConsistency ë©”ëª¨ë¦¬ ì¼ê´€ì„± í…ŒìŠ¤íŠ¸
func TestMemoryConsistency(t *testing.T) {
	tester := NewConcurrencyTester(t)
	defer tester.cacheService.Close()
	
	t.Run("Sequential Consistency", func(t *testing.T) {
		tester.testSequentialConsistency(t)
	})
	
	t.Run("Eventual Consistency", func(t *testing.T) {
		tester.testEventualConsistency(t)
	})
	
	t.Run("Causal Consistency", func(t *testing.T) {
		tester.testCausalConsistency(t)
	})
}

// testSequentialConsistency ìˆœì°¨ ì¼ê´€ì„± í…ŒìŠ¤íŠ¸
func (ct *ConcurrencyTester) testSequentialConsistency(t *testing.T) {
	t.Log("ğŸ” Testing Sequential Consistency")
	
	const (
		numOperations = 100
		numReaders    = 10
		testKey       = "sequential_test"
	)
	
	var wg sync.WaitGroup
	var operations []string
	var readings [][]string
	var mu sync.Mutex
	
	// ì´ˆê¸°ê°’ ì„¤ì •
	err := ct.cacheService.Set(ct.ctx, testKey, "operation_0", time.Hour)
	require.NoError(t, err)
	
	start := time.Now()
	
	// ìˆœì°¨ì  ì“°ê¸° ì‘ì—…
	wg.Add(1)
	go func() {
		defer wg.Done()
		for i := 1; i <= numOperations; i++ {
			value := fmt.Sprintf("operation_%d", i)
			err := ct.cacheService.Set(ct.ctx, testKey, value, time.Hour)
			if err == nil {
				mu.Lock()
				operations = append(operations, value)
				mu.Unlock()
			}
			time.Sleep(time.Millisecond) // ìˆœì°¨ì„± ë³´ì¥ì„ ìœ„í•œ ì§€ì—°
		}
	}()
	
	// ë™ì‹œ ì½ê¸° ì‘ì—…ë“¤
	for r := 0; r < numReaders; r++ {
		wg.Add(1)
		go func(readerID int) {
			defer wg.Done()
			
			var readerValues []string
			for i := 0; i < numOperations/2; i++ {
				value, err := ct.cacheService.Get(ct.ctx, testKey)
				if err == nil {
					readerValues = append(readerValues, value)
				}
				time.Sleep(time.Millisecond * 2)
			}
			
			mu.Lock()
			readings = append(readings, readerValues)
			mu.Unlock()
		}(r)
	}
	
	wg.Wait()
	duration := time.Since(start)
	
	// ìˆœì°¨ ì¼ê´€ì„± ë¶„ì„
	violations := ct.analyzeSequentialConsistency(operations, readings)
	
	t.Logf("=== Sequential Consistency Results ===")
	t.Logf("Total Operations: %d", len(operations))
	t.Logf("Readers: %d", numReaders)
	t.Logf("Consistency Violations: %d", violations)
	t.Logf("Duration: %v", duration)
	
	if violations > 0 {
		t.Logf("ğŸ”´ Sequential consistency violations detected: %d", violations)
		ct.metrics.DataInconsistencies += int64(violations)
	}
}

// analyzeSequentialConsistency ìˆœì°¨ ì¼ê´€ì„± ë¶„ì„
func (ct *ConcurrencyTester) analyzeSequentialConsistency(operations []string, readings [][]string) int {
	violations := 0
	
	// ê° ë¦¬ë”ì˜ ì½ê¸° ì‹œí€€ìŠ¤ ë¶„ì„
	for readerID, readerValues := range readings {
		for i := 1; i < len(readerValues); i++ {
			currentVal := readerValues[i]
			prevVal := readerValues[i-1]
			
			// ì´ì „ ê°’ë³´ë‹¤ ì‘ì€ ì¸ë±ìŠ¤ì˜ ê°’ì´ ë‚˜íƒ€ë‚˜ë©´ ìˆœì°¨ì„± ìœ„ë°˜
			currentIdx := ct.findOperationIndex(operations, currentVal)
			prevIdx := ct.findOperationIndex(operations, prevVal)
			
			if currentIdx >= 0 && prevIdx >= 0 && currentIdx < prevIdx {
				violations++
				// ìì„¸í•œ ë¡œê¹…ì€ í…ŒìŠ¤íŠ¸ì—ì„œ ì œì–´
			}
		}
	}
	
	return violations
}

// findOperationIndex ì‘ì—… ì¸ë±ìŠ¤ ì°¾ê¸°
func (ct *ConcurrencyTester) findOperationIndex(operations []string, value string) int {
	for i, op := range operations {
		if op == value {
			return i
		}
	}
	return -1
}

// testEventualConsistency ìµœì¢… ì¼ê´€ì„± í…ŒìŠ¤íŠ¸
func (ct *ConcurrencyTester) testEventualConsistency(t *testing.T) {
	t.Log("ğŸ” Testing Eventual Consistency")
	
	const (
		numUpdaters    = 5
		updatesPerUser = 10
		testKey        = "eventual_test"
	)
	
	var wg sync.WaitGroup
	var finalValues []string
	var mu sync.Mutex
	
	// ë™ì‹œ ì—…ë°ì´íŠ¸
	for u := 0; u < numUpdaters; u++ {
		wg.Add(1)
		go func(updaterID int) {
			defer wg.Done()
			
			for i := 0; i < updatesPerUser; i++ {
				value := fmt.Sprintf("updater_%d_value_%d", updaterID, i)
				err := ct.cacheService.Set(ct.ctx, testKey, value, time.Hour)
				if err == nil {
					mu.Lock()
					finalValues = append(finalValues, value)
					mu.Unlock()
				}
				time.Sleep(time.Millisecond * 5)
			}
		}(u)
	}
	
	wg.Wait()
	
	// ìµœì¢… ì¼ê´€ì„± í™•ì¸ì„ ìœ„í•œ ëŒ€ê¸°
	time.Sleep(100 * time.Millisecond)
	
	// ìµœì¢… ê°’ í™•ì¸
	finalValue, err := ct.cacheService.Get(ct.ctx, testKey)
	require.NoError(t, err)
	
	// ìµœì¢… ê°’ì´ ìˆ˜í–‰ëœ ì—…ë°ì´íŠ¸ ì¤‘ í•˜ë‚˜ì¸ì§€ í™•ì¸
	isConsistent := false
	for _, value := range finalValues {
		if value == finalValue {
			isConsistent = true
			break
		}
	}
	
	t.Logf("=== Eventual Consistency Results ===")
	t.Logf("Total Updates: %d", len(finalValues))
	t.Logf("Final Value: %s", finalValue)
	t.Logf("Is Eventually Consistent: %t", isConsistent)
	
	assert.True(t, isConsistent, "Eventual consistency violation")
}

// testCausalConsistency ì¸ê³¼ ì¼ê´€ì„± í…ŒìŠ¤íŠ¸
func (ct *ConcurrencyTester) testCausalConsistency(t *testing.T) {
	t.Log("ğŸ” Testing Causal Consistency")
	
	const testKey = "causal_test"
	
	// ì¸ê³¼ê´€ê³„ê°€ ìˆëŠ” ì‘ì—… ì‹œí€€ìŠ¤
	var wg sync.WaitGroup
	
	// ì²« ë²ˆì§¸ ì‘ì—…
	err := ct.cacheService.Set(ct.ctx, testKey, "cause_1", time.Hour)
	require.NoError(t, err)
	
	time.Sleep(10 * time.Millisecond) // ì¸ê³¼ê´€ê³„ ë³´ì¥ì„ ìœ„í•œ ì§€ì—°
	
	// ë‘ ë²ˆì§¸ ì‘ì—… (ì²« ë²ˆì§¸ì— ì˜ì¡´)
	wg.Add(1)
	go func() {
		defer wg.Done()
		// ì²« ë²ˆì§¸ ê°’ ì½ê¸°
		value, err := ct.cacheService.Get(ct.ctx, testKey)
		if err == nil && value == "cause_1" {
			// ì¸ê³¼ê´€ê³„ì— ë”°ë¥¸ ì—…ë°ì´íŠ¸
			ct.cacheService.Set(ct.ctx, testKey, "effect_1", time.Hour)
		}
	}()
	
	// ì„¸ ë²ˆì§¸ ì‘ì—… (ë‘ ë²ˆì§¸ì— ì˜ì¡´)
	wg.Add(1)
	go func() {
		defer wg.Done()
		time.Sleep(20 * time.Millisecond) // ë‘ ë²ˆì§¸ ì‘ì—… í›„ ì‹¤í–‰ ë³´ì¥
		
		value, err := ct.cacheService.Get(ct.ctx, testKey)
		if err == nil && value == "effect_1" {
			ct.cacheService.Set(ct.ctx, testKey, "effect_2", time.Hour)
		}
	}()
	
	wg.Wait()
	
	// ìµœì¢… ì¸ê³¼ê´€ê³„ ê²€ì¦
	finalValue, err := ct.cacheService.Get(ct.ctx, testKey)
	require.NoError(t, err)
	
	t.Logf("=== Causal Consistency Results ===")
	t.Logf("Final Value: %s", finalValue)
	
	// ì¸ê³¼ê´€ê³„ ê²€ì¦
	expectedSequence := []string{"cause_1", "effect_1", "effect_2"}
	isValid := false
	for _, expected := range expectedSequence {
		if finalValue == expected {
			isValid = true
			break
		}
	}
	
	t.Logf("Causal Consistency Maintained: %t", isValid)
	assert.True(t, isValid, "Causal consistency violation")
}

// TestConcurrencySummary ë™ì‹œì„± í…ŒìŠ¤íŠ¸ ì¢…í•© ë³´ê³ ì„œ
func TestConcurrencySummary(t *testing.T) {
	tester := NewConcurrencyTester(t)
	defer tester.cacheService.Close()
	
	t.Log("ğŸ“Š Concurrency Test Summary Report")
	
	// ì „ì²´ ë©”íŠ¸ë¦­ ìš”ì•½
	t.Logf("=== Concurrency Issues Summary ===")
	t.Logf("Race Conditions: %d", tester.metrics.RaceConditions)
	t.Logf("Data Inconsistencies: %d", tester.metrics.DataInconsistencies)
	t.Logf("Cache Conflicts: %d", tester.metrics.CacheConflicts)
	t.Logf("Deadlock Attempts: %d", tester.metrics.DeadlockAttempts)
	t.Logf("Lost Updates: %d", tester.metrics.LostUpdates)
	t.Logf("Data Corruptions: %d", tester.metrics.DataCorruptions)
	
	// ì‹¬ê°ë„ë³„ ë¶„ë¥˜
	criticalIssues := tester.metrics.DeadlockAttempts + tester.metrics.DataCorruptions
	majorIssues := tester.metrics.RaceConditions + tester.metrics.DataInconsistencies
	minorIssues := tester.metrics.CacheConflicts + tester.metrics.LostUpdates
	
	t.Logf("=== Issue Severity Analysis ===")
	t.Logf("Critical Issues: %d", criticalIssues)
	t.Logf("Major Issues: %d", majorIssues)
	t.Logf("Minor Issues: %d", minorIssues)
	
	// ê°œì„  ê¶Œê³ ì‚¬í•­
	t.Logf("=== Improvement Recommendations ===")
	if tester.metrics.RaceConditions > 0 {
		t.Logf("ğŸ”´ Implement proper synchronization mechanisms")
	}
	if tester.metrics.DeadlockAttempts > 0 {
		t.Logf("ğŸ”´ Add deadlock detection and recovery")
	}
	if tester.metrics.DataInconsistencies > 0 {
		t.Logf("ğŸŸ¡ Review consistency models and isolation levels")
	}
	if tester.metrics.CacheConflicts > 0 {
		t.Logf("ğŸŸ¡ Optimize cache invalidation strategies")
	}
	
	// ì „ì²´ í‰ê°€
	totalIssues := criticalIssues + majorIssues + minorIssues
	if totalIssues == 0 {
		t.Logf("âœ… No significant concurrency issues detected")
	} else if totalIssues < 10 {
		t.Logf("ğŸŸ¡ Minor concurrency issues detected - monitoring recommended")
	} else {
		t.Logf("ğŸ”´ Significant concurrency issues detected - immediate attention required")
	}
}