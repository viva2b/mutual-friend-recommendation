package stress

import (
	"context"
	"fmt"
	"runtime"
	"sync"
	"sync/atomic"
	"testing"
	"time"

	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"

	"mutual-friend/pkg/cache"
	"mutual-friend/pkg/config"
	"mutual-friend/pkg/redis"
)

// StressTester ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ êµ¬ì¡°ì²´
type StressTester struct {
	cacheService cache.Cache
	ctx          context.Context
	metrics      *StressMetrics
}

// StressMetrics ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ë©”íŠ¸ë¦­
type StressMetrics struct {
	mu                    sync.RWMutex
	
	// ë¶€í•˜ í…ŒìŠ¤íŠ¸ ë©”íŠ¸ë¦­
	TotalRequests         int64
	SuccessfulRequests    int64
	FailedRequests        int64
	TimeoutRequests       int64
	
	// ì‘ë‹µ ì‹œê°„ ë©”íŠ¸ë¦­
	ResponseTimes         []time.Duration
	MinResponseTime       time.Duration
	MaxResponseTime       time.Duration
	AvgResponseTime       time.Duration
	
	// ì²˜ë¦¬ëŸ‰ ë©”íŠ¸ë¦­
	RequestsPerSecond     float64
	PeakThroughput        float64
	SustainedThroughput   float64
	
	// ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ë©”íŠ¸ë¦­
	MemoryUsage           []uint64
	CPUUsage              []float64
	GoroutineCount        []int
	ConnectionCount       []int
	
	// ì—ëŸ¬ íŒ¨í„´ ë©”íŠ¸ë¦­
	ConnectionErrors      int64
	TimeoutErrors         int64
	MemoryErrors          int64
	ThrottlingErrors      int64
	
	// ì•ˆì •ì„± ë©”íŠ¸ë¦­
	SystemStability       float64  // 0-100%
	ErrorRate             float64  // 0-100%
	RecoveryTime          time.Duration
	
	// í™•ì¥ì„± ë©”íŠ¸ë¦­
	ScalabilityIndex      float64  // ì„±ëŠ¥ ëŒ€ë¹„ ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„±
	BottleneckComponents  []string
	
	// ì¥ì•  ë‚´ì„± ë©”íŠ¸ë¦­
	FailureRecoveryTime   time.Duration
	DataLossEvents        int64
	ServiceDegradation    float64
}

func (sm *StressMetrics) RecordRequest(success bool, responseTime time.Duration) {
	sm.mu.Lock()
	defer sm.mu.Unlock()
	
	atomic.AddInt64(&sm.TotalRequests, 1)
	
	if success {
		atomic.AddInt64(&sm.SuccessfulRequests, 1)
	} else {
		atomic.AddInt64(&sm.FailedRequests, 1)
	}
	
	sm.ResponseTimes = append(sm.ResponseTimes, responseTime)
	
	if sm.MinResponseTime == 0 || responseTime < sm.MinResponseTime {
		sm.MinResponseTime = responseTime
	}
	if responseTime > sm.MaxResponseTime {
		sm.MaxResponseTime = responseTime
	}
}

func (sm *StressMetrics) RecordResourceUsage() {
	sm.mu.Lock()
	defer sm.mu.Unlock()
	
	var memStats runtime.MemStats
	runtime.ReadMemStats(&memStats)
	
	sm.MemoryUsage = append(sm.MemoryUsage, memStats.Alloc)
	sm.GoroutineCount = append(sm.GoroutineCount, runtime.NumGoroutine())
}

func (sm *StressMetrics) CalculateMetrics() {
	sm.mu.Lock()
	defer sm.mu.Unlock()
	
	// í‰ê·  ì‘ë‹µ ì‹œê°„ ê³„ì‚°
	if len(sm.ResponseTimes) > 0 {
		var total time.Duration
		for _, rt := range sm.ResponseTimes {
			total += rt
		}
		sm.AvgResponseTime = total / time.Duration(len(sm.ResponseTimes))
	}
	
	// ì—ëŸ¬ìœ¨ ê³„ì‚°
	if sm.TotalRequests > 0 {
		sm.ErrorRate = float64(sm.FailedRequests) / float64(sm.TotalRequests) * 100
	}
	
	// ì‹œìŠ¤í…œ ì•ˆì •ì„± ê³„ì‚° (ì—ëŸ¬ìœ¨ ê¸°ë°˜)
	sm.SystemStability = 100.0 - sm.ErrorRate
}

// NewStressTester ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤í„° ìƒì„±
func NewStressTester(t *testing.T) *StressTester {
	// ì„¤ì • ë¡œë“œ
	cfg, err := config.LoadConfig("../../configs/config.yaml")
	require.NoError(t, err)
	
	// Redis í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
	redisClient, err := redis.NewClient(&redis.Config{
		Address:      cfg.Redis.Address,
		Password:     cfg.Redis.Password,
		Database:     cfg.Redis.Database,
		PoolSize:     cfg.Redis.PoolSize,
		MinIdleConns: cfg.Redis.MinIdleConns,
		MaxRetries:   cfg.Redis.MaxRetries,
		DialTimeout:  time.Duration(cfg.Redis.DialTimeout) * time.Second,
		ReadTimeout:  time.Duration(cfg.Redis.ReadTimeout) * time.Second,
		WriteTimeout: time.Duration(cfg.Redis.WriteTimeout) * time.Second,
	})
	require.NoError(t, err)
	
	// ìºì‹œ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
	cacheService, err := cache.NewService(redisClient)
	require.NoError(t, err)
	
	return &StressTester{
		cacheService: cacheService,
		ctx:          context.Background(),
		metrics:      &StressMetrics{},
	}
}

// TestLoadTesting ë¶€í•˜ í…ŒìŠ¤íŠ¸
func TestLoadTesting(t *testing.T) {
	tester := NewStressTester(t)
	defer tester.cacheService.Close()
	
	t.Run("Gradual Load Increase", func(t *testing.T) {
		tester.testGradualLoadIncrease(t)
	})
	
	t.Run("Sustained High Load", func(t *testing.T) {
		tester.testSustainedHighLoad(t)
	})
	
	t.Run("Spike Load Testing", func(t *testing.T) {
		tester.testSpikeLoad(t)
	})
	
	t.Run("Memory Pressure Testing", func(t *testing.T) {
		tester.testMemoryPressure(t)
	})
}

// testGradualLoadIncrease ì ì§„ì  ë¶€í•˜ ì¦ê°€ í…ŒìŠ¤íŠ¸
func (st *StressTester) testGradualLoadIncrease(t *testing.T) {
	t.Log("ğŸ” Testing Gradual Load Increase")
	
	// ë¶€í•˜ ë‹¨ê³„ë³„ ì„¤ì •
	loadStages := []struct {
		name        string
		concurrency int
		duration    time.Duration
		rps         int // requests per second target
	}{
		{"Warm-up", 10, 30 * time.Second, 100},
		{"Light Load", 50, 60 * time.Second, 500},
		{"Medium Load", 100, 60 * time.Second, 1000},
		{"Heavy Load", 200, 60 * time.Second, 2000},
		{"Peak Load", 500, 60 * time.Second, 5000},
	}
	
	for _, stage := range loadStages {
		t.Run(stage.name, func(t *testing.T) {
			st.runLoadStage(t, stage.name, stage.concurrency, stage.duration, stage.rps)
		})
	}
	
	// ì „ì²´ ê²°ê³¼ ë¶„ì„
	st.analyzeLoadTestResults(t)
}

// runLoadStage ë¶€í•˜ ë‹¨ê³„ ì‹¤í–‰
func (st *StressTester) runLoadStage(t *testing.T, stageName string, concurrency int, duration time.Duration, targetRPS int) {
	t.Logf("Running %s: %d concurrent workers, %v duration, target %d RPS", 
		stageName, concurrency, duration, targetRPS)
	
	var wg sync.WaitGroup
	ctx, cancel := context.WithTimeout(st.ctx, duration)
	defer cancel()
	
	// ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘
	monitoringDone := make(chan bool)
	go st.monitorResources(monitoringDone)
	
	start := time.Now()
	
	// ì›Œì»¤ ê³ ë£¨í‹´ ì‹œì‘
	for i := 0; i < concurrency; i++ {
		wg.Add(1)
		go func(workerID int) {
			defer wg.Done()
			st.worker(ctx, workerID, targetRPS/concurrency)
		}(i)
	}
	
	wg.Wait()
	close(monitoringDone)
	actualDuration := time.Since(start)
	
	// ë‹¨ê³„ë³„ ê²°ê³¼ ê³„ì‚°
	st.metrics.CalculateMetrics()
	
	t.Logf("=== %s Results ===", stageName)
	t.Logf("Duration: %v", actualDuration)
	t.Logf("Total Requests: %d", st.metrics.TotalRequests)
	t.Logf("Successful: %d", st.metrics.SuccessfulRequests)
	t.Logf("Failed: %d", st.metrics.FailedRequests)
	t.Logf("Error Rate: %.2f%%", st.metrics.ErrorRate)
	t.Logf("Avg Response Time: %v", st.metrics.AvgResponseTime)
	t.Logf("Min/Max Response Time: %v/%v", st.metrics.MinResponseTime, st.metrics.MaxResponseTime)
	
	actualRPS := float64(st.metrics.TotalRequests) / actualDuration.Seconds()
	t.Logf("Actual RPS: %.2f", actualRPS)
	
	// ì„±ëŠ¥ ì„ê³„ê°’ ê²€ì¦
	if st.metrics.ErrorRate > 5.0 {
		t.Logf("âš ï¸  High error rate in %s: %.2f%%", stageName, st.metrics.ErrorRate)
	}
	if st.metrics.AvgResponseTime > 100*time.Millisecond {
		t.Logf("âš ï¸  High response time in %s: %v", stageName, st.metrics.AvgResponseTime)
	}
}

// worker ì›Œì»¤ ê³ ë£¨í‹´
func (st *StressTester) worker(ctx context.Context, workerID int, targetRPS int) {
	requestInterval := time.Second / time.Duration(targetRPS)
	ticker := time.NewTicker(requestInterval)
	defer ticker.Stop()
	
	for {
		select {
		case <-ctx.Done():
			return
		case <-ticker.C:
			st.executeRequest(workerID)
		}
	}
}

// executeRequest ìš”ì²­ ì‹¤í–‰
func (st *StressTester) executeRequest(workerID int) {
	start := time.Now()
	
	key := fmt.Sprintf("stress_test_worker_%d_req_%d", workerID, time.Now().UnixNano())
	value := fmt.Sprintf("test_value_%d", time.Now().UnixNano())
	
	// 50% SET, 50% GET ì‘ì—…
	var err error
	if time.Now().UnixNano()%2 == 0 {
		// SET ì‘ì—…
		err = st.cacheService.Set(st.ctx, key, value, time.Minute)
	} else {
		// GET ì‘ì—… (ëœë¤ í‚¤)
		randomKey := fmt.Sprintf("stress_test_worker_%d_req_%d", 
			workerID, time.Now().UnixNano()-int64(workerID*1000))
		_, err = st.cacheService.Get(st.ctx, randomKey)
	}
	
	responseTime := time.Since(start)
	success := err == nil
	
	st.metrics.RecordRequest(success, responseTime)
	
	if err != nil {
		// ì—ëŸ¬ íƒ€ì…ë³„ ë¶„ë¥˜
		atomic.AddInt64(&st.metrics.ConnectionErrors, 1)
	}
}

// monitorResources ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§
func (st *StressTester) monitorResources(done chan bool) {
	ticker := time.NewTicker(5 * time.Second)
	defer ticker.Stop()
	
	for {
		select {
		case <-done:
			return
		case <-ticker.C:
			st.metrics.RecordResourceUsage()
		}
	}
}

// analyzeLoadTestResults ë¶€í•˜ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„
func (st *StressTester) analyzeLoadTestResults(t *testing.T) {
	t.Log("ğŸ“Š Load Test Results Analysis")
	
	st.metrics.CalculateMetrics()
	
	t.Logf("=== Overall Performance Summary ===")
	t.Logf("Total Requests: %d", st.metrics.TotalRequests)
	t.Logf("Success Rate: %.2f%%", 100.0-st.metrics.ErrorRate)
	t.Logf("Average Response Time: %v", st.metrics.AvgResponseTime)
	t.Logf("Response Time Range: %v - %v", st.metrics.MinResponseTime, st.metrics.MaxResponseTime)
	
	// ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ë¶„ì„
	if len(st.metrics.MemoryUsage) > 0 {
		maxMemory := uint64(0)
		for _, mem := range st.metrics.MemoryUsage {
			if mem > maxMemory {
				maxMemory = mem
			}
		}
		t.Logf("Peak Memory Usage: %d MB", maxMemory/(1024*1024))
	}
	
	if len(st.metrics.GoroutineCount) > 0 {
		maxGoroutines := 0
		for _, count := range st.metrics.GoroutineCount {
			if count > maxGoroutines {
				maxGoroutines = count
			}
		}
		t.Logf("Peak Goroutine Count: %d", maxGoroutines)
	}
	
	// ì„±ëŠ¥ í‰ê°€
	if st.metrics.ErrorRate < 1.0 {
		t.Logf("âœ… Excellent error rate: %.2f%%", st.metrics.ErrorRate)
	} else if st.metrics.ErrorRate < 5.0 {
		t.Logf("ğŸŸ¡ Acceptable error rate: %.2f%%", st.metrics.ErrorRate)
	} else {
		t.Logf("ğŸ”´ High error rate: %.2f%%", st.metrics.ErrorRate)
	}
	
	if st.metrics.AvgResponseTime < 50*time.Millisecond {
		t.Logf("âœ… Excellent response time: %v", st.metrics.AvgResponseTime)
	} else if st.metrics.AvgResponseTime < 200*time.Millisecond {
		t.Logf("ğŸŸ¡ Acceptable response time: %v", st.metrics.AvgResponseTime)
	} else {
		t.Logf("ğŸ”´ High response time: %v", st.metrics.AvgResponseTime)
	}
}

// testSustainedHighLoad ì§€ì†ì  ê³ ë¶€í•˜ í…ŒìŠ¤íŠ¸
func (st *StressTester) testSustainedHighLoad(t *testing.T) {
	t.Log("ğŸ” Testing Sustained High Load")
	
	const (
		concurrency = 1000
		duration    = 5 * time.Minute
		targetRPS   = 10000
	)
	
	t.Logf("Running sustained load: %d workers, %v duration, target %d RPS", 
		concurrency, duration, targetRPS)
	
	// ì´ˆê¸° ë©”íŠ¸ë¦­ ì´ˆê¸°í™”
	st.metrics = &StressMetrics{}
	
	var wg sync.WaitGroup
	ctx, cancel := context.WithTimeout(st.ctx, duration)
	defer cancel()
	
	// ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
	monitoringDone := make(chan bool)
	go st.monitorSustainedLoad(monitoringDone)
	
	start := time.Now()
	
	// ê³ ë¶€í•˜ ì›Œì»¤ë“¤ ì‹œì‘
	for i := 0; i < concurrency; i++ {
		wg.Add(1)
		go func(workerID int) {
			defer wg.Done()
			st.sustainedLoadWorker(ctx, workerID, targetRPS/concurrency)
		}(i)
	}
	
	wg.Wait()
	close(monitoringDone)
	actualDuration := time.Since(start)
	
	st.metrics.CalculateMetrics()
	
	t.Logf("=== Sustained High Load Results ===")
	t.Logf("Actual Duration: %v", actualDuration)
	t.Logf("Total Requests: %d", st.metrics.TotalRequests)
	t.Logf("Requests/Second: %.2f", float64(st.metrics.TotalRequests)/actualDuration.Seconds())
	t.Logf("Error Rate: %.2f%%", st.metrics.ErrorRate)
	t.Logf("System Stability: %.2f%%", st.metrics.SystemStability)
	t.Logf("Average Response Time: %v", st.metrics.AvgResponseTime)
	
	// ì§€ì†ì„± ê²€ì¦
	assert.Less(t, st.metrics.ErrorRate, 2.0, "Error rate too high for sustained load")
	assert.Less(t, st.metrics.AvgResponseTime, 500*time.Millisecond, "Response time degraded under sustained load")
	assert.Greater(t, st.metrics.SystemStability, 95.0, "System not stable under sustained load")
}

// sustainedLoadWorker ì§€ì†ì  ë¶€í•˜ ì›Œì»¤
func (st *StressTester) sustainedLoadWorker(ctx context.Context, workerID int, targetRPS int) {
	requestInterval := time.Second / time.Duration(targetRPS)
	ticker := time.NewTicker(requestInterval)
	defer ticker.Stop()
	
	requestCount := 0
	
	for {
		select {
		case <-ctx.Done():
			return
		case <-ticker.C:
			start := time.Now()
			
			// ë‹¤ì–‘í•œ ì‘ì—… íŒ¨í„´
			var err error
			switch requestCount % 4 {
			case 0: // SET
				key := fmt.Sprintf("sustained_set_%d_%d", workerID, requestCount)
				value := fmt.Sprintf("value_%d_%d", workerID, time.Now().UnixNano())
				err = st.cacheService.Set(st.ctx, key, value, time.Hour)
				
			case 1: // GET (existing)
				key := fmt.Sprintf("sustained_set_%d_%d", workerID, requestCount-1)
				_, err = st.cacheService.Get(st.ctx, key)
				
			case 2: // GET (non-existing)
				key := fmt.Sprintf("non_existing_%d_%d", workerID, requestCount)
				_, err = st.cacheService.Get(st.ctx, key)
				
			case 3: // DELETE
				key := fmt.Sprintf("sustained_set_%d_%d", workerID, requestCount-3)
				_, err = st.cacheService.Delete(st.ctx, key)
			}
			
			responseTime := time.Since(start)
			st.metrics.RecordRequest(err == nil, responseTime)
			
			requestCount++
			
			// ë©”ëª¨ë¦¬ ì••ë°• ê°ì§€
			if requestCount%1000 == 0 {
				var memStats runtime.MemStats
				runtime.ReadMemStats(&memStats)
				if memStats.Alloc > 1024*1024*1024 { // 1GB ì´ˆê³¼
					atomic.AddInt64(&st.metrics.MemoryErrors, 1)
				}
			}
		}
	}
}

// monitorSustainedLoad ì§€ì†ì  ë¶€í•˜ ëª¨ë‹ˆí„°ë§
func (st *StressTester) monitorSustainedLoad(done chan bool) {
	ticker := time.NewTicker(10 * time.Second)
	defer ticker.Stop()
	
	for {
		select {
		case <-done:
			return
		case <-ticker.C:
			st.metrics.RecordResourceUsage()
			
			// ì²˜ë¦¬ëŸ‰ ê³„ì‚°
			currentRPS := float64(atomic.LoadInt64(&st.metrics.TotalRequests)) / time.Since(time.Now()).Seconds()
			if currentRPS > st.metrics.PeakThroughput {
				st.metrics.PeakThroughput = currentRPS
			}
		}
	}
}

// testSpikeLoad ìŠ¤íŒŒì´í¬ ë¶€í•˜ í…ŒìŠ¤íŠ¸
func (st *StressTester) testSpikeLoad(t *testing.T) {
	t.Log("ğŸ” Testing Spike Load")
	
	// ìŠ¤íŒŒì´í¬ íŒ¨í„´: ë‚®ì€ ë¶€í•˜ â†’ ê¸‰ê²©í•œ ì¦ê°€ â†’ ë†’ì€ ë¶€í•˜ â†’ ê¸‰ê²©í•œ ê°ì†Œ
	phases := []struct {
		name        string
		concurrency int
		duration    time.Duration
	}{
		{"Baseline", 50, 30 * time.Second},
		{"Spike Up", 2000, 60 * time.Second},
		{"Peak Load", 2000, 120 * time.Second},
		{"Spike Down", 50, 30 * time.Second},
	}
	
	st.metrics = &StressMetrics{}
	
	for _, phase := range phases {
		t.Run(phase.name, func(t *testing.T) {
			st.runSpikePhase(t, phase.name, phase.concurrency, phase.duration)
		})
	}
	
	// ìŠ¤íŒŒì´í¬ ë³µêµ¬ ë¶„ì„
	st.analyzeSpikeRecovery(t)
}

// runSpikePhase ìŠ¤íŒŒì´í¬ ë‹¨ê³„ ì‹¤í–‰
func (st *StressTester) runSpikePhase(t *testing.T, phaseName string, concurrency int, duration time.Duration) {
	t.Logf("Running %s phase: %d workers for %v", phaseName, concurrency, duration)
	
	var wg sync.WaitGroup
	ctx, cancel := context.WithTimeout(st.ctx, duration)
	defer cancel()
	
	phaseStart := time.Now()
	
	// ìŠ¤íŒŒì´í¬ ì›Œì»¤ë“¤
	for i := 0; i < concurrency; i++ {
		wg.Add(1)
		go func(workerID int) {
			defer wg.Done()
			
			for {
				select {
				case <-ctx.Done():
					return
				default:
					start := time.Now()
					
					key := fmt.Sprintf("spike_%s_%d_%d", phaseName, workerID, time.Now().UnixNano())
					value := fmt.Sprintf("spike_value_%d", time.Now().UnixNano())
					
					err := st.cacheService.Set(st.ctx, key, value, time.Minute)
					responseTime := time.Since(start)
					
					st.metrics.RecordRequest(err == nil, responseTime)
					
					// ìŠ¤íŒŒì´í¬ ì¤‘ ì—ëŸ¬ íŒ¨í„´ ì¶”ì 
					if err != nil {
						if responseTime > 10*time.Second {
							atomic.AddInt64(&st.metrics.TimeoutErrors, 1)
						} else {
							atomic.AddInt64(&st.metrics.ConnectionErrors, 1)
						}
					}
					
					// ìŠ¤íŒŒì´í¬ ì¤‘ ë” ê³µê²©ì ì¸ ìš”ì²­
					if phaseName == "Spike Up" || phaseName == "Peak Load" {
						time.Sleep(time.Microsecond * 100)
					} else {
						time.Sleep(time.Millisecond * 10)
					}
				}
			}
		}(i)
	}
	
	wg.Wait()
	phaseDuration := time.Since(phaseStart)
	
	t.Logf("%s phase completed in %v", phaseName, phaseDuration)
}

// analyzeSpikeRecovery ìŠ¤íŒŒì´í¬ ë³µêµ¬ ë¶„ì„
func (st *StressTester) analyzeSpikeRecovery(t *testing.T) {
	t.Log("ğŸ“Š Spike Load Recovery Analysis")
	
	st.metrics.CalculateMetrics()
	
	t.Logf("=== Spike Load Test Results ===")
	t.Logf("Total Requests: %d", st.metrics.TotalRequests)
	t.Logf("Overall Error Rate: %.2f%%", st.metrics.ErrorRate)
	t.Logf("Timeout Errors: %d", st.metrics.TimeoutErrors)
	t.Logf("Connection Errors: %d", st.metrics.ConnectionErrors)
	t.Logf("Peak Response Time: %v", st.metrics.MaxResponseTime)
	
	// ë³µêµ¬ ëŠ¥ë ¥ í‰ê°€
	if st.metrics.ErrorRate < 10.0 {
		t.Logf("âœ… Good spike load handling")
	} else if st.metrics.ErrorRate < 25.0 {
		t.Logf("ğŸŸ¡ Moderate spike load handling")
	} else {
		t.Logf("ğŸ”´ Poor spike load handling")
	}
	
	// ë³µêµ¬ ì‹œê°„ ì¶”ì • (ë§ˆì§€ë§‰ ë‹¨ê³„ì˜ ì‘ë‹µ ì‹œê°„ ê¸°ë°˜)
	if len(st.metrics.ResponseTimes) > 100 {
		lastResponses := st.metrics.ResponseTimes[len(st.metrics.ResponseTimes)-100:]
		var totalRecoveryTime time.Duration
		for _, rt := range lastResponses {
			totalRecoveryTime += rt
		}
		avgRecoveryTime := totalRecoveryTime / time.Duration(len(lastResponses))
		
		t.Logf("Recovery Response Time: %v", avgRecoveryTime)
		st.metrics.RecoveryTime = avgRecoveryTime
	}
}

// testMemoryPressure ë©”ëª¨ë¦¬ ì••ë°• í…ŒìŠ¤íŠ¸
func (st *StressTester) testMemoryPressure(t *testing.T) {
	t.Log("ğŸ” Testing Memory Pressure")
	
	const (
		largeValueSize = 1024 * 1024 // 1MB per value
		numLargeValues = 100
		concurrency    = 50
	)
	
	st.metrics = &StressMetrics{}
	
	// ëŒ€ìš©ëŸ‰ ë°ì´í„° ìƒì„±
	largeValue := make([]byte, largeValueSize)
	for i := range largeValue {
		largeValue[i] = byte(i % 256)
	}
	largeValueStr := string(largeValue)
	
	var wg sync.WaitGroup
	ctx, cancel := context.WithTimeout(st.ctx, 2*time.Minute)
	defer cancel()
	
	// ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì‹œì‘
	monitoringDone := make(chan bool)
	go st.monitorMemoryPressure(monitoringDone)
	
	start := time.Now()
	
	// ë©”ëª¨ë¦¬ ì••ë°• ì›Œì»¤ë“¤
	for i := 0; i < concurrency; i++ {
		wg.Add(1)
		go func(workerID int) {
			defer wg.Done()
			
			for j := 0; j < numLargeValues/concurrency; j++ {
				select {
				case <-ctx.Done():
					return
				default:
					reqStart := time.Now()
					
					key := fmt.Sprintf("memory_pressure_%d_%d", workerID, j)
					err := st.cacheService.Set(st.ctx, key, largeValueStr, time.Hour)
					
					responseTime := time.Since(reqStart)
					st.metrics.RecordRequest(err == nil, responseTime)
					
					if err != nil {
						atomic.AddInt64(&st.metrics.MemoryErrors, 1)
					}
					
					// ë©”ëª¨ë¦¬ ì••ë°• ìƒí™©ì—ì„œì˜ ì½ê¸° í…ŒìŠ¤íŠ¸
					_, err = st.cacheService.Get(st.ctx, key)
					if err != nil {
						atomic.AddInt64(&st.metrics.MemoryErrors, 1)
					}
					
					time.Sleep(time.Millisecond * 100)
				}
			}
		}(i)
	}
	
	wg.Wait()
	close(monitoringDone)
	duration := time.Since(start)
	
	st.metrics.CalculateMetrics()
	
	t.Logf("=== Memory Pressure Test Results ===")
	t.Logf("Duration: %v", duration)
	t.Logf("Large Values Stored: %d", numLargeValues)
	t.Logf("Total Data Size: %d MB", (numLargeValues*largeValueSize)/(1024*1024))
	t.Logf("Memory Errors: %d", st.metrics.MemoryErrors)
	t.Logf("Error Rate: %.2f%%", st.metrics.ErrorRate)
	t.Logf("Average Response Time: %v", st.metrics.AvgResponseTime)
	
	// ë©”ëª¨ë¦¬ ì••ë°• í…ŒìŠ¤íŠ¸ ê²€ì¦
	if st.metrics.MemoryErrors == 0 {
		t.Logf("âœ… No memory-related errors")
	} else if st.metrics.MemoryErrors < 10 {
		t.Logf("ğŸŸ¡ Some memory pressure detected: %d errors", st.metrics.MemoryErrors)
	} else {
		t.Logf("ğŸ”´ Significant memory pressure: %d errors", st.metrics.MemoryErrors)
	}
	
	assert.Less(t, st.metrics.ErrorRate, 15.0, "Too many errors under memory pressure")
}

// monitorMemoryPressure ë©”ëª¨ë¦¬ ì••ë°• ëª¨ë‹ˆí„°ë§
func (st *StressTester) monitorMemoryPressure(done chan bool) {
	ticker := time.NewTicker(5 * time.Second)
	defer ticker.Stop()
	
	for {
		select {
		case <-done:
			return
		case <-ticker.C:
			var memStats runtime.MemStats
			runtime.ReadMemStats(&memStats)
			
			st.metrics.mu.Lock()
			st.metrics.MemoryUsage = append(st.metrics.MemoryUsage, memStats.Alloc)
			st.metrics.GoroutineCount = append(st.metrics.GoroutineCount, runtime.NumGoroutine())
			st.metrics.mu.Unlock()
			
			// ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì„ê³„ì¹˜ë¥¼ ë„˜ìœ¼ë©´ ê²½ê³ 
			if memStats.Alloc > 2*1024*1024*1024 { // 2GB
				atomic.AddInt64(&st.metrics.MemoryErrors, 1)
			}
		}
	}
}

// TestFailureRecovery ì¥ì•  ë³µêµ¬ í…ŒìŠ¤íŠ¸
func TestFailureRecovery(t *testing.T) {
	tester := NewStressTester(t)
	defer tester.cacheService.Close()
	
	t.Run("Connection Failure Recovery", func(t *testing.T) {
		tester.testConnectionFailureRecovery(t)
	})
	
	t.Run("Timeout Recovery", func(t *testing.T) {
		tester.testTimeoutRecovery(t)
	})
	
	t.Run("Resource Exhaustion Recovery", func(t *testing.T) {
		tester.testResourceExhaustionRecovery(t)
	})
}

// testConnectionFailureRecovery ì—°ê²° ì¥ì•  ë³µêµ¬ í…ŒìŠ¤íŠ¸
func (st *StressTester) testConnectionFailureRecovery(t *testing.T) {
	t.Log("ğŸ” Testing Connection Failure Recovery")
	
	const (
		normalOps    = 1000
		concurrency  = 100
	)
	
	st.metrics = &StressMetrics{}
	
	var wg sync.WaitGroup
	ctx, cancel := context.WithTimeout(st.ctx, 2*time.Minute)
	defer cancel()
	
	failureStart := time.Now().Add(30 * time.Second) // 30ì´ˆ í›„ ì¥ì•  ì‹œë®¬ë ˆì´ì…˜
	recoveryTime := time.Time{}
	
	for i := 0; i < concurrency; i++ {
		wg.Add(1)
		go func(workerID int) {
			defer wg.Done()
			
			for j := 0; j < normalOps/concurrency; j++ {
				select {
				case <-ctx.Done():
					return
				default:
					start := time.Now()
					
					// ì¥ì•  ì‹œë®¬ë ˆì´ì…˜ (íŠ¹ì • ì‹œê°„ëŒ€ì— ê°•ì œ ì§€ì—°)
					if start.After(failureStart) && start.Before(failureStart.Add(20*time.Second)) {
						if recoveryTime.IsZero() {
							recoveryTime = start.Add(20 * time.Second)
						}
						// ì¥ì•  ìƒí™© ì‹œë®¬ë ˆì´ì…˜ (ì—°ê²° ì§€ì—°)
						time.Sleep(time.Millisecond * 500)
					}
					
					key := fmt.Sprintf("recovery_test_%d_%d", workerID, j)
					value := fmt.Sprintf("value_%d", time.Now().UnixNano())
					
					err := st.cacheService.Set(st.ctx, key, value, time.Minute)
					responseTime := time.Since(start)
					
					st.metrics.RecordRequest(err == nil, responseTime)
					
					if err != nil {
						atomic.AddInt64(&st.metrics.ConnectionErrors, 1)
					}
					
					time.Sleep(time.Millisecond * 10)
				}
			}
		}(i)
	}
	
	wg.Wait()
	st.metrics.CalculateMetrics()
	
	// ë³µêµ¬ ì‹œê°„ ì¸¡ì •
	if !recoveryTime.IsZero() {
		st.metrics.FailureRecoveryTime = time.Since(recoveryTime)
	}
	
	t.Logf("=== Connection Failure Recovery Results ===")
	t.Logf("Total Requests: %d", st.metrics.TotalRequests)
	t.Logf("Connection Errors: %d", st.metrics.ConnectionErrors)
	t.Logf("Error Rate: %.2f%%", st.metrics.ErrorRate)
	t.Logf("Recovery Time: %v", st.metrics.FailureRecoveryTime)
	t.Logf("System Stability: %.2f%%", st.metrics.SystemStability)
	
	// ë³µêµ¬ ëŠ¥ë ¥ ê²€ì¦
	if st.metrics.ErrorRate < 20.0 {
		t.Logf("âœ… Good failure recovery")
	} else {
		t.Logf("ğŸ”´ Poor failure recovery")
	}
	
	assert.Less(t, st.metrics.ErrorRate, 30.0, "Poor connection failure recovery")
}

// testTimeoutRecovery íƒ€ì„ì•„ì›ƒ ë³µêµ¬ í…ŒìŠ¤íŠ¸
func (st *StressTester) testTimeoutRecovery(t *testing.T) {
	t.Log("ğŸ” Testing Timeout Recovery")
	
	// ì§§ì€ íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ì„¤ì •í•˜ì—¬ íƒ€ì„ì•„ì›ƒ ìƒí™© ìœ ë„
	// ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” Redis í´ë¼ì´ì–¸íŠ¸ íƒ€ì„ì•„ì›ƒ ì„¤ì •ì„ ì¡°ì •í•´ì•¼ í•¨
	
	const (
		operations   = 500
		concurrency  = 50
	)
	
	st.metrics = &StressMetrics{}
	
	var wg sync.WaitGroup
	
	for i := 0; i < concurrency; i++ {
		wg.Add(1)
		go func(workerID int) {
			defer wg.Done()
			
			for j := 0; j < operations/concurrency; j++ {
				start := time.Now()
				
				// í° ë°ì´í„°ë¡œ íƒ€ì„ì•„ì›ƒ ìœ ë„
				largeData := make([]byte, 1024*1024) // 1MB
				key := fmt.Sprintf("timeout_test_%d_%d", workerID, j)
				
				err := st.cacheService.Set(st.ctx, key, string(largeData), time.Minute)
				responseTime := time.Since(start)
				
				st.metrics.RecordRequest(err == nil, responseTime)
				
				if err != nil {
					if responseTime > 5*time.Second {
						atomic.AddInt64(&st.metrics.TimeoutErrors, 1)
					}
				}
				
				time.Sleep(time.Millisecond * 20)
			}
		}(i)
	}
	
	wg.Wait()
	st.metrics.CalculateMetrics()
	
	t.Logf("=== Timeout Recovery Results ===")
	t.Logf("Total Requests: %d", st.metrics.TotalRequests)
	t.Logf("Timeout Errors: %d", st.metrics.TimeoutErrors)
	t.Logf("Error Rate: %.2f%%", st.metrics.ErrorRate)
	t.Logf("Max Response Time: %v", st.metrics.MaxResponseTime)
	
	if st.metrics.TimeoutErrors < 50 {
		t.Logf("âœ… Good timeout handling")
	} else {
		t.Logf("ğŸ”´ Many timeout errors: %d", st.metrics.TimeoutErrors)
	}
}

// testResourceExhaustionRecovery ë¦¬ì†ŒìŠ¤ ê³ ê°ˆ ë³µêµ¬ í…ŒìŠ¤íŠ¸
func (st *StressTester) testResourceExhaustionRecovery(t *testing.T) {
	t.Log("ğŸ” Testing Resource Exhaustion Recovery")
	
	// ì˜ë„ì ìœ¼ë¡œ ë§ì€ ê³ ë£¨í‹´ê³¼ ë©”ëª¨ë¦¬ ì‚¬ìš©
	const (
		extremeConcurrency = 5000
		duration          = 30 * time.Second
	)
	
	st.metrics = &StressMetrics{}
	
	var wg sync.WaitGroup
	ctx, cancel := context.WithTimeout(st.ctx, duration)
	defer cancel()
	
	start := time.Now()
	
	// ê·¹í•œ ë™ì‹œì„±ìœ¼ë¡œ ë¦¬ì†ŒìŠ¤ ê³ ê°ˆ ìœ ë„
	for i := 0; i < extremeConcurrency; i++ {
		wg.Add(1)
		go func(workerID int) {
			defer wg.Done()
			
			// ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€
			buffer := make([]byte, 1024*100) // 100KB per goroutine
			
			for j := 0; j < 10; j++ {
				select {
				case <-ctx.Done():
					return
				default:
					reqStart := time.Now()
					
					key := fmt.Sprintf("exhaustion_%d_%d", workerID, j)
					value := fmt.Sprintf("%s_%d", string(buffer[:100]), j)
					
					err := st.cacheService.Set(st.ctx, key, value, time.Minute)
					responseTime := time.Since(reqStart)
					
					st.metrics.RecordRequest(err == nil, responseTime)
					
					time.Sleep(time.Millisecond * 100)
				}
			}
		}(i)
	}
	
	wg.Wait()
	actualDuration := time.Since(start)
	
	st.metrics.CalculateMetrics()
	
	t.Logf("=== Resource Exhaustion Recovery Results ===")
	t.Logf("Extreme Concurrency: %d goroutines", extremeConcurrency)
	t.Logf("Duration: %v", actualDuration)
	t.Logf("Total Requests: %d", st.metrics.TotalRequests)
	t.Logf("Error Rate: %.2f%%", st.metrics.ErrorRate)
	t.Logf("Average Response Time: %v", st.metrics.AvgResponseTime)
	
	// ì‹œìŠ¤í…œì´ ì™„ì „íˆ ì¤‘ë‹¨ë˜ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸
	if st.metrics.TotalRequests > 0 {
		t.Logf("âœ… System remained responsive under extreme load")
	} else {
		t.Logf("ğŸ”´ System became unresponsive")
	}
	
	assert.Greater(t, st.metrics.TotalRequests, int64(0), "System became completely unresponsive")
}

// TestStressSummary ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì¢…í•© ë³´ê³ ì„œ
func TestStressSummary(t *testing.T) {
	tester := NewStressTester(t)
	defer tester.cacheService.Close()
	
	t.Log("ğŸ“Š Stress Test Summary Report")
	
	// ì¢…í•© ë©”íŠ¸ë¦­ ê³„ì‚°
	tester.metrics.CalculateMetrics()
	
	t.Logf("=== Overall Stress Test Performance ===")
	t.Logf("Peak Throughput: %.2f RPS", tester.metrics.PeakThroughput)
	t.Logf("Sustained Throughput: %.2f RPS", tester.metrics.SustainedThroughput)
	t.Logf("System Stability: %.2f%%", tester.metrics.SystemStability)
	t.Logf("Recovery Time: %v", tester.metrics.RecoveryTime)
	
	t.Logf("=== Error Analysis ===")
	t.Logf("Connection Errors: %d", tester.metrics.ConnectionErrors)
	t.Logf("Timeout Errors: %d", tester.metrics.TimeoutErrors)
	t.Logf("Memory Errors: %d", tester.metrics.MemoryErrors)
	t.Logf("Throttling Errors: %d", tester.metrics.ThrottlingErrors)
	
	// í™•ì¥ì„± ë¶„ì„
	if len(tester.metrics.MemoryUsage) > 0 && len(tester.metrics.ResponseTimes) > 0 {
		// ê°„ë‹¨í•œ í™•ì¥ì„± ì§€ìˆ˜ ê³„ì‚°
		avgMemory := uint64(0)
		for _, mem := range tester.metrics.MemoryUsage {
			avgMemory += mem
		}
		avgMemory /= uint64(len(tester.metrics.MemoryUsage))
		
		avgResponseTime := tester.metrics.AvgResponseTime.Nanoseconds()
		
		// ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
		memoryEfficiency := float64(avgMemory) / float64(tester.metrics.TotalRequests)
		
		// ì‘ë‹µ ì‹œê°„ íš¨ìœ¨ì„± (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
		timeEfficiency := float64(avgResponseTime) / float64(tester.metrics.TotalRequests)
		
		// ì¢…í•© í™•ì¥ì„± ì§€ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
		tester.metrics.ScalabilityIndex = 100.0 / (memoryEfficiency + timeEfficiency) * 1000000
		
		t.Logf("=== Scalability Analysis ===")
		t.Logf("Scalability Index: %.2f", tester.metrics.ScalabilityIndex)
		t.Logf("Memory Efficiency: %.2f bytes/request", memoryEfficiency)
		t.Logf("Time Efficiency: %.2f ns/request", timeEfficiency)
	}
	
	// ë³‘ëª© ì§€ì  ì‹ë³„
	bottlenecks := []string{}
	if tester.metrics.MemoryErrors > 0 {
		bottlenecks = append(bottlenecks, "Memory")
	}
	if tester.metrics.ConnectionErrors > tester.metrics.TimeoutErrors {
		bottlenecks = append(bottlenecks, "Connection Pool")
	}
	if tester.metrics.TimeoutErrors > 0 {
		bottlenecks = append(bottlenecks, "Network/Processing")
	}
	
	tester.metrics.BottleneckComponents = bottlenecks
	
	t.Logf("=== Identified Bottlenecks ===")
	if len(bottlenecks) == 0 {
		t.Logf("âœ… No significant bottlenecks identified")
	} else {
		for _, bottleneck := range bottlenecks {
			t.Logf("ğŸ”´ Bottleneck: %s", bottleneck)
		}
	}
	
	// ì „ì²´ í‰ê°€
	if tester.metrics.SystemStability > 95.0 && tester.metrics.ScalabilityIndex > 50.0 {
		t.Logf("âœ… Excellent stress test performance")
	} else if tester.metrics.SystemStability > 90.0 && tester.metrics.ScalabilityIndex > 25.0 {
		t.Logf("ğŸŸ¡ Good stress test performance")
	} else {
		t.Logf("ğŸ”´ Poor stress test performance - optimization needed")
	}
	
	// ê°œì„  ê¶Œê³ ì‚¬í•­
	t.Logf("=== Improvement Recommendations ===")
	if tester.metrics.MemoryErrors > 0 {
		t.Logf("ğŸ”´ Implement memory management and garbage collection optimization")
	}
	if tester.metrics.ConnectionErrors > 50 {
		t.Logf("ğŸ”´ Increase connection pool size and implement connection retry logic")
	}
	if tester.metrics.TimeoutErrors > 50 {
		t.Logf("ğŸŸ¡ Optimize processing logic and consider async patterns")
	}
	if tester.metrics.RecoveryTime > 30*time.Second {
		t.Logf("ğŸŸ¡ Implement faster failure detection and recovery mechanisms")
	}
}