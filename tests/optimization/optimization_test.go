package optimization

import (
	"fmt"
	"testing"
	"time"

	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"

	"mutual-friend/pkg/cache"
	"mutual-friend/pkg/config"
	"mutual-friend/pkg/redis"
)

// TestSystemOptimization ÏãúÏä§ÌÖú ÏµúÏ†ÅÌôî ÌÖåÏä§Ìä∏
func TestSystemOptimization(t *testing.T) {
	// ÏÑ§Ï†ï Î°úÎìú
	cfg, err := config.LoadConfig("../../configs/config.yaml")
	require.NoError(t, err)
	
	// Redis ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ï¥àÍ∏∞Ìôî
	redisClient, err := redis.NewClient(&redis.Config{
		Address:      cfg.Redis.Address,
		Password:     cfg.Redis.Password,
		Database:     cfg.Redis.Database,
		PoolSize:     cfg.Redis.PoolSize,
		MinIdleConns: cfg.Redis.MinIdleConns,
		MaxRetries:   cfg.Redis.MaxRetries,
		DialTimeout:  time.Duration(cfg.Redis.DialTimeout) * time.Second,
		ReadTimeout:  time.Duration(cfg.Redis.ReadTimeout) * time.Second,
		WriteTimeout: time.Duration(cfg.Redis.WriteTimeout) * time.Second,
	})
	require.NoError(t, err)
	
	// Ï∫êÏãú ÏÑúÎπÑÏä§ Ï¥àÍ∏∞Ìôî
	cacheService, err := cache.NewService(redisClient)
	require.NoError(t, err)
	defer cacheService.Close()
	
	t.Run("Complete Optimization Workflow", func(t *testing.T) {
		testCompleteOptimizationWorkflow(t, cacheService)
	})
	
	t.Run("Performance Issue Identification", func(t *testing.T) {
		testPerformanceIssueIdentification(t, cacheService)
	})
	
	t.Run("Optimization Strategy Selection", func(t *testing.T) {
		testOptimizationStrategySelection(t, cacheService)
	})
	
	t.Run("Impact Analysis", func(t *testing.T) {
		testOptimizationImpactAnalysis(t, cacheService)
	})
}

// testCompleteOptimizationWorkflow ÏôÑÏ†ÑÌïú ÏµúÏ†ÅÌôî ÏõåÌÅ¨ÌîåÎ°úÏö∞ ÌÖåÏä§Ìä∏
func testCompleteOptimizationWorkflow(t *testing.T, cacheService cache.Cache) {
	t.Log("üîç Testing Complete Optimization Workflow")
	
	// ÏãúÏä§ÌÖú ÏµúÏ†ÅÌôîÍ∏∞ ÏÉùÏÑ±
	optimizer := NewSystemOptimizer(cacheService)
	
	// 1Îã®Í≥Ñ: Í∏∞Ï§Ä ÏÑ±Îä• Ï∏°Ï†ï
	t.Log("üìä Step 1: Measuring baseline performance...")
	err := optimizer.MeasureBaseline()
	require.NoError(t, err)
	
	baseline := optimizer.originalMetrics
	t.Logf("=== Baseline Performance ===")
	t.Logf("Average Response Time: %v", baseline.AvgResponseTime)
	t.Logf("Throughput: %.1f RPS", baseline.Throughput)
	t.Logf("Error Rate: %.2f%%", baseline.ErrorRate)
	t.Logf("Memory Usage: %d MB", baseline.MemoryUsage/(1024*1024))
	t.Logf("Cache Hit Rate: %.1f%%", baseline.CacheHitRate)
	t.Logf("Goroutine Count: %d", baseline.GoroutineCount)
	
	// 2Îã®Í≥Ñ: ÏÑ±Îä• Î¨∏Ï†ú ÏãùÎ≥Ñ
	t.Log("üîç Step 2: Identifying performance issues...")
	issues := optimizer.IdentifyPerformanceIssues()
	
	t.Logf("=== Performance Issues Identified ===")
	if len(issues) == 0 {
		t.Logf("‚úÖ No significant performance issues detected")
	} else {
		for _, issue := range issues {
			severity := "üü°"
			if issue.Severity == "critical" {
				severity = "üî¥"
			} else if issue.Severity == "high" {
				severity = "üü†"
			}
			
			t.Logf("%s %s: %s (Impact: %.1f%%, Priority: %d)", 
				severity, issue.Type, issue.Description, issue.Impact, issue.Priority)
			t.Logf("   Solution: %s", issue.Solution)
		}
	}
	
	// 3Îã®Í≥Ñ: ÏµúÏ†ÅÌôî Ï†ÑÎûµ ÏÑ†ÌÉù
	t.Log("üéØ Step 3: Selecting optimization strategies...")
	strategies := optimizer.GetOptimizationStrategies()
	
	// Í≥†ÏûÑÌå©Ìä∏, Ï†ÄÏúÑÌóò Ï†ÑÎûµ Ïö∞ÏÑ† ÏÑ†ÌÉù
	selectedStrategies := selectOptimalStrategies(strategies)
	
	t.Logf("=== Selected Optimization Strategies ===")
	for _, strategy := range selectedStrategies {
		riskColor := "‚úÖ"
		if strategy.RiskLevel == "medium" {
			riskColor = "üü°"
		} else if strategy.RiskLevel == "high" {
			riskColor = "üî¥"
		}
		
		t.Logf("%s %s (Expected: %.1f%%, Risk: %s)", 
			riskColor, strategy.Name, strategy.ExpectedGain, strategy.RiskLevel)
		t.Logf("   %s", strategy.Description)
	}
	
	// 4Îã®Í≥Ñ: ÏµúÏ†ÅÌôî Ï†ÅÏö©
	t.Log("‚ö° Step 4: Applying optimizations...")
	err = optimizer.ApplyOptimizations(selectedStrategies)
	require.NoError(t, err)
	
	// 5Îã®Í≥Ñ: ÏµúÏ†ÅÌôî ÌõÑ ÏÑ±Îä• Ï∏°Ï†ï
	t.Log("üìà Step 5: Measuring optimized performance...")
	err = optimizer.MeasureOptimizedPerformance()
	require.NoError(t, err)
	
	optimized := optimizer.optimizedMetrics
	t.Logf("=== Optimized Performance ===")
	t.Logf("Average Response Time: %v", optimized.AvgResponseTime)
	t.Logf("Throughput: %.1f RPS", optimized.Throughput)
	t.Logf("Error Rate: %.2f%%", optimized.ErrorRate)
	t.Logf("Memory Usage: %d MB", optimized.MemoryUsage/(1024*1024))
	t.Logf("Cache Hit Rate: %.1f%%", optimized.CacheHitRate)
	t.Logf("Goroutine Count: %d", optimized.GoroutineCount)
	
	// 6Îã®Í≥Ñ: Í∞úÏÑ† ÏÇ¨Ìï≠ Î∂ÑÏÑù
	t.Log("üìä Step 6: Analyzing improvements...")
	t.Logf("=== Performance Improvements ===")
	t.Logf("Response Time: %.1f%% improvement", optimized.ResponseTimeImprovement)
	t.Logf("Throughput: %.1f%% improvement", optimized.ThroughputImprovement)
	t.Logf("Memory Efficiency: %.1f%% improvement", optimized.MemoryEfficiency)
	t.Logf("Overall Improvement: %.1f%%", optimized.OverallImprovement)
	
	// 7Îã®Í≥Ñ: ÏµúÏ†ÅÌôî Î≥¥Í≥†ÏÑú ÏÉùÏÑ±
	t.Log("üìã Step 7: Generating optimization report...")
	report := optimizer.GenerateOptimizationReport()
	
	t.Logf("=== Optimization Summary ===")
	t.Logf("Total Issues Found: %d", report.Summary.TotalIssuesFound)
	t.Logf("Critical Issues: %d", report.Summary.CriticalIssues)
	t.Logf("Strategies Applied: %d", report.Summary.StrategiesApplied)
	t.Logf("Overall Improvement: %.1f%%", report.Summary.OverallImprovement)
	t.Logf("Business Value: %s", report.Summary.EstimatedBusinessValue)
	
	if len(report.Summary.RecommendedNextSteps) > 0 {
		t.Logf("Recommended Next Steps:")
		for _, step := range report.Summary.RecommendedNextSteps {
			t.Logf("  - %s", step)
		}
	}
	
	// Í≤ÄÏ¶ù
	assert.NotZero(t, baseline.Throughput, "Baseline throughput should be measured")
	assert.NotZero(t, optimized.Throughput, "Optimized throughput should be measured")
	
	// ÏÑ±Îä• Í∞úÏÑ† Í≤ÄÏ¶ù (ÏãúÎÆ¨Î†àÏù¥ÏÖòÏóêÏÑúÎäî ÏùºÎ∂Ä Í∞úÏÑ†Ïù¥ Î≥¥Ïû•ÎêòÏßÄ ÏïäÏùÑ Ïàò ÏûàÏùå)
	if optimized.OverallImprovement > 0 {
		t.Logf("‚úÖ Performance improvement achieved: %.1f%%", optimized.OverallImprovement)
	} else {
		t.Logf("‚ö†Ô∏è  No significant improvement detected (expected in simulation)")
	}
}

// selectOptimalStrategies ÏµúÏ†Å Ï†ÑÎûµ ÏÑ†ÌÉù
func selectOptimalStrategies(strategies []OptimizationStrategy) []OptimizationStrategy {
	var selected []OptimizationStrategy
	
	// ÏúÑÌóò-ÏàòÏùµ ÎπÑÏú®ÏùÑ Í≥†Î†§ÌïòÏó¨ Ï†ÑÎûµ ÏÑ†ÌÉù
	for _, strategy := range strategies {
		riskScore := 1.0
		if strategy.RiskLevel == "medium" {
			riskScore = 2.0
		} else if strategy.RiskLevel == "high" {
			riskScore = 3.0
		}
		
		// Í∏∞ÎåÄ ÏàòÏùµÏù¥ ÏúÑÌóòÎ≥¥Îã§ Ï∂©Î∂ÑÌûà ÌÅ∞ Ï†ÑÎûµÎßå ÏÑ†ÌÉù
		riskRewardRatio := strategy.ExpectedGain / riskScore
		if riskRewardRatio >= 10.0 { // ÏµúÏÜå 10:1 ÎπÑÏú®
			selected = append(selected, strategy)
		}
	}
	
	// ÏµúÏÜåÌïú Ï†ÄÏúÑÌóò Ï†ÑÎûµÏùÄ Ìè¨Ìï®
	if len(selected) == 0 {
		for _, strategy := range strategies {
			if strategy.RiskLevel == "low" {
				selected = append(selected, strategy)
				break
			}
		}
	}
	
	return selected
}

// testPerformanceIssueIdentification ÏÑ±Îä• Î¨∏Ï†ú ÏãùÎ≥Ñ ÌÖåÏä§Ìä∏
func testPerformanceIssueIdentification(t *testing.T, cacheService cache.Cache) {
	t.Log("üîç Testing Performance Issue Identification")
	
	optimizer := NewSystemOptimizer(cacheService)
	
	// ÏÑ±Îä• Î¨∏Ï†úÍ∞Ä ÏûàÎäî ÏãúÎÇòÎ¶¨Ïò§ ÏãúÎÆ¨Î†àÏù¥ÏÖò
	optimizer.originalMetrics = &BaselineMetrics{
		AvgResponseTime:     800 * time.Millisecond, // ÎÜíÏùÄ ÏùëÎãµ ÏãúÍ∞Ñ
		Throughput:          300,                     // ÎÇÆÏùÄ Ï≤òÎ¶¨Îüâ
		ErrorRate:           3.5,                     // ÎÜíÏùÄ ÏóêÎü¨Ïú®
		MemoryUsage:         800 * 1024 * 1024,       // ÎÜíÏùÄ Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Îüâ
		CacheHitRate:        75.0,                    // ÎÇÆÏùÄ Ï∫êÏãú ÌûàÌä∏Ïú®
		ConnectionPoolUsage: 90.0,                    // ÎÜíÏùÄ Ïó∞Í≤∞ ÌíÄ ÏÇ¨Ïö©Î•†
		GoroutineCount:      2000,                    // ÎßéÏùÄ Í≥†Î£®Ìã¥
	}
	
	issues := optimizer.IdentifyPerformanceIssues()
	
	t.Logf("=== Performance Issue Analysis ===")
	
	// Î¨∏Ï†ú Ïú†ÌòïÎ≥Ñ Î∂ÑÎ•ò
	issueTypes := make(map[string]int)
	severityCounts := make(map[string]int)
	var totalImpact float64
	
	for _, issue := range issues {
		issueTypes[issue.Type]++
		severityCounts[issue.Severity]++
		totalImpact += issue.Impact
		
		t.Logf("Issue: %s (%s)", issue.Description, issue.Severity)
		t.Logf("  Type: %s, Impact: %.1f%%, Priority: %d", 
			issue.Type, issue.Impact, issue.Priority)
		t.Logf("  Solution: %s", issue.Solution)
	}
	
	t.Logf("=== Issue Summary ===")
	t.Logf("Total Issues: %d", len(issues))
	t.Logf("Total Impact: %.1f%%", totalImpact)
	
	for issueType, count := range issueTypes {
		t.Logf("%s issues: %d", issueType, count)
	}
	
	for severity, count := range severityCounts {
		t.Logf("%s severity: %d", severity, count)
	}
	
	// Í≤ÄÏ¶ù
	assert.Greater(t, len(issues), 0, "Should identify performance issues with poor metrics")
	assert.Contains(t, issueTypes, "latency", "Should identify latency issues")
	assert.Contains(t, issueTypes, "throughput", "Should identify throughput issues")
	assert.Greater(t, severityCounts["high"], 0, "Should identify high severity issues")
}

// testOptimizationStrategySelection ÏµúÏ†ÅÌôî Ï†ÑÎûµ ÏÑ†ÌÉù ÌÖåÏä§Ìä∏
func testOptimizationStrategySelection(t *testing.T, cacheService cache.Cache) {
	t.Log("üéØ Testing Optimization Strategy Selection")
	
	optimizer := NewSystemOptimizer(cacheService)
	strategies := optimizer.GetOptimizationStrategies()
	
	t.Logf("=== Available Strategies ===")
	for _, strategy := range strategies {
		t.Logf("Strategy: %s", strategy.Name)
		t.Logf("  Expected Gain: %.1f%%", strategy.ExpectedGain)
		t.Logf("  Risk Level: %s", strategy.RiskLevel)
		t.Logf("  Resource Impact: %s", strategy.ResourceImpact)
		t.Logf("  Description: %s", strategy.Description)
	}
	
	// Ï†ÑÎûµ Î∂ÑÏÑù
	riskLevels := make(map[string]int)
	var totalExpectedGain float64
	
	for _, strategy := range strategies {
		riskLevels[strategy.RiskLevel]++
		totalExpectedGain += strategy.ExpectedGain
	}
	
	avgExpectedGain := totalExpectedGain / float64(len(strategies))
	
	t.Logf("=== Strategy Analysis ===")
	t.Logf("Total Strategies: %d", len(strategies))
	t.Logf("Average Expected Gain: %.1f%%", avgExpectedGain)
	for risk, count := range riskLevels {
		t.Logf("%s risk strategies: %d", risk, count)
	}
	
	// Ï†ÑÎûµ ÏÑ†ÌÉù ÏãúÎÆ¨Î†àÏù¥ÏÖò
	selectedStrategies := selectOptimalStrategies(strategies)
	
	t.Logf("=== Selected Strategies ===")
	var selectedGain float64
	for _, strategy := range selectedStrategies {
		t.Logf("‚úÖ %s (%.1f%% gain, %s risk)", 
			strategy.Name, strategy.ExpectedGain, strategy.RiskLevel)
		selectedGain += strategy.ExpectedGain
	}
	
	t.Logf("Total Expected Improvement: %.1f%%", selectedGain)
	
	// Í≤ÄÏ¶ù
	assert.Greater(t, len(strategies), 5, "Should have multiple optimization strategies")
	assert.Greater(t, len(selectedStrategies), 0, "Should select at least one strategy")
	assert.Greater(t, avgExpectedGain, 10.0, "Strategies should have meaningful expected gains")
}

// testOptimizationImpactAnalysis ÏµúÏ†ÅÌôî ÏòÅÌñ• Î∂ÑÏÑù ÌÖåÏä§Ìä∏
func testOptimizationImpactAnalysis(t *testing.T, cacheService cache.Cache) {
	t.Log("üìä Testing Optimization Impact Analysis")
	
	optimizer := NewSystemOptimizer(cacheService)
	
	// ÏµúÏ†ÅÌôî Ï†ÑÌõÑ ÏãúÎÇòÎ¶¨Ïò§ ÏãúÎÆ¨Î†àÏù¥ÏÖò
	optimizer.originalMetrics = &BaselineMetrics{
		AvgResponseTime:     500 * time.Millisecond,
		Throughput:          800,
		ErrorRate:           2.0,
		MemoryUsage:         400 * 1024 * 1024,
		CacheHitRate:        85.0,
		ConnectionPoolUsage: 75.0,
		GoroutineCount:      500,
	}
	
	optimizer.optimizedMetrics = &OptimizedMetrics{
		AvgResponseTime:     300 * time.Millisecond,
		Throughput:          1200,
		ErrorRate:           0.5,
		MemoryUsage:         300 * 1024 * 1024,
		CacheHitRate:        95.0,
		ConnectionPoolUsage: 60.0,
		GoroutineCount:      400,
	}
	
	// Í∞úÏÑ† ÏÇ¨Ìï≠ Í≥ÑÏÇ∞
	optimizer.calculateImprovements()
	
	optimized := optimizer.optimizedMetrics
	
	t.Logf("=== Optimization Impact Analysis ===")
	t.Logf("Response Time Improvement: %.1f%%", optimized.ResponseTimeImprovement)
	t.Logf("Throughput Improvement: %.1f%%", optimized.ThroughputImprovement)
	t.Logf("Memory Efficiency: %.1f%%", optimized.MemoryEfficiency)
	t.Logf("Overall Improvement: %.1f%%", optimized.OverallImprovement)
	
	// ÏÉÅÏÑ∏ Î∂ÑÏÑù
	t.Logf("=== Detailed Impact ===")
	t.Logf("Response Time: %v ‚Üí %v", 
		optimizer.originalMetrics.AvgResponseTime, optimized.AvgResponseTime)
	t.Logf("Throughput: %.1f ‚Üí %.1f RPS", 
		optimizer.originalMetrics.Throughput, optimized.Throughput)
	t.Logf("Error Rate: %.1f%% ‚Üí %.1f%%", 
		optimizer.originalMetrics.ErrorRate, optimized.ErrorRate)
	t.Logf("Memory: %d ‚Üí %d MB", 
		optimizer.originalMetrics.MemoryUsage/(1024*1024), 
		optimized.MemoryUsage/(1024*1024))
	t.Logf("Cache Hit Rate: %.1f%% ‚Üí %.1f%%", 
		optimizer.originalMetrics.CacheHitRate, optimized.CacheHitRate)
	
	// ÎπÑÏ¶àÎãàÏä§ ÏòÅÌñ• Î∂ÑÏÑù
	responseTimeGain := optimized.ResponseTimeImprovement
	throughputGain := optimized.ThroughputImprovement
	
	estimatedUserExperienceImprovement := responseTimeGain * 0.8 // ÏùëÎãµÏãúÍ∞ÑÏùò 80%Í∞Ä UX ÏòÅÌñ•
	estimatedCapacityIncrease := throughputGain
	
	t.Logf("=== Business Impact Estimation ===")
	t.Logf("User Experience Improvement: %.1f%%", estimatedUserExperienceImprovement)
	t.Logf("System Capacity Increase: %.1f%%", estimatedCapacityIncrease)
	
	if optimized.OverallImprovement > 30 {
		t.Logf("üí∞ High business value optimization")
	} else if optimized.OverallImprovement > 15 {
		t.Logf("üíµ Medium business value optimization")
	} else {
		t.Logf("üí¥ Low business value optimization")
	}
	
	// ROI Ï∂îÏ†ï (Îã®ÏàúÌôîÎêú Í≥ÑÏÇ∞)
	implementationCost := 100.0 // ÏûÑÏùòÏùò Í∏∞Ï§ÄÍ∞í
	monthlyBenefit := optimized.OverallImprovement * 0.1 // Í∞úÏÑ†ÎèÑÏùò 10%Î•º ÏõîÍ∞Ñ Ïù¥ÏùµÏúºÎ°ú Í∞ÄÏ†ï
	roi := (monthlyBenefit * 12 - implementationCost) / implementationCost * 100
	
	t.Logf("Estimated ROI: %.1f%% (12 months)", roi)
	
	// Í≤ÄÏ¶ù
	assert.Greater(t, optimized.ResponseTimeImprovement, 0.0, "Should show response time improvement")
	assert.Greater(t, optimized.ThroughputImprovement, 0.0, "Should show throughput improvement")
	assert.Greater(t, optimized.OverallImprovement, 0.0, "Should show overall improvement")
}

// TestOptimizationRecommendations ÏµúÏ†ÅÌôî Í∂åÍ≥†ÏÇ¨Ìï≠ ÌÖåÏä§Ìä∏
func TestOptimizationRecommendations(t *testing.T) {
	t.Log("üí° Testing Optimization Recommendations")
	
	// Îã§ÏñëÌïú ÏÑ±Îä• ÏãúÎÇòÎ¶¨Ïò§Ïóê ÎåÄÌïú Í∂åÍ≥†ÏÇ¨Ìï≠ ÌÖåÏä§Ìä∏
	scenarios := []struct {
		name     string
		metrics  BaselineMetrics
		expected []string // ÏòàÏÉÅÎêòÎäî Í∂åÍ≥†ÏÇ¨Ìï≠ ÌÇ§ÏõåÎìú
	}{
		{
			name: "High Latency Scenario",
			metrics: BaselineMetrics{
				AvgResponseTime: 2 * time.Second,
				Throughput:      500,
				ErrorRate:       1.0,
			},
			expected: []string{"connection", "cache", "optimization"},
		},
		{
			name: "Low Throughput Scenario",
			metrics: BaselineMetrics{
				AvgResponseTime: 100 * time.Millisecond,
				Throughput:      100,
				ErrorRate:       0.5,
			},
			expected: []string{"scaling", "async", "pool"},
		},
		{
			name: "High Error Rate Scenario",
			metrics: BaselineMetrics{
				AvgResponseTime: 200 * time.Millisecond,
				Throughput:      800,
				ErrorRate:       8.0,
			},
			expected: []string{"error", "circuit", "retry"},
		},
	}
	
	for _, scenario := range scenarios {
		t.Run(scenario.name, func(t *testing.T) {
			optimizer := &SystemOptimizer{
				originalMetrics: &scenario.metrics,
			}
			
			issues := optimizer.IdentifyPerformanceIssues()
			strategies := optimizer.GetOptimizationStrategies()
			
			t.Logf("=== %s ===", scenario.name)
			t.Logf("Issues found: %d", len(issues))
			t.Logf("Available strategies: %d", len(strategies))
			
			// Í∂åÍ≥†ÏÇ¨Ìï≠ ÌôïÏù∏
			var recommendations []string
			for _, issue := range issues {
				recommendations = append(recommendations, issue.Solution)
			}
			
			for _, strategy := range strategies {
				recommendations = append(recommendations, strategy.Description)
			}
			
			t.Logf("Recommendations generated: %d", len(recommendations))
			
			// ÏòàÏÉÅ ÌÇ§ÏõåÎìú Ìè¨Ìï® Ïó¨Î∂Ä ÌôïÏù∏
			foundKeywords := make(map[string]bool)
			for _, expected := range scenario.expected {
				for _, rec := range recommendations {
					if contains(rec, expected) {
						foundKeywords[expected] = true
						break
					}
				}
			}
			
			t.Logf("Expected keywords found: %d/%d", len(foundKeywords), len(scenario.expected))
			
			assert.Greater(t, len(issues), 0, "Should identify issues for problematic scenarios")
		})
	}
}

// contains Î¨∏ÏûêÏó¥ Ìè¨Ìï® Ïó¨Î∂Ä ÌôïÏù∏ (Í∞ÑÎã®Ìïú Í≤ÄÏÉâ)
func contains(text, substr string) bool {
	return len(text) >= len(substr) && 
		   (text == substr || 
		    (len(text) > len(substr) && 
		     (text[:len(substr)] == substr || 
		      text[len(text)-len(substr):] == substr ||
		      findSubstring(text, substr))))
}

func findSubstring(text, substr string) bool {
	for i := 0; i <= len(text)-len(substr); i++ {
		if text[i:i+len(substr)] == substr {
			return true
		}
	}
	return false
}

// TestOptimizationSummary ÏµúÏ†ÅÌôî Ï¢ÖÌï© Î≥¥Í≥†ÏÑú ÌÖåÏä§Ìä∏
func TestOptimizationSummary(t *testing.T) {
	t.Log("üìã Optimization Summary Report")
	
	// Ï¢ÖÌï©Ï†ÅÏù∏ ÏµúÏ†ÅÌôî ÏãúÎÇòÎ¶¨Ïò§
	optimizer := &SystemOptimizer{
		originalMetrics: &BaselineMetrics{
			AvgResponseTime:     400 * time.Millisecond,
			Throughput:          600,
			ErrorRate:           2.5,
			MemoryUsage:         500 * 1024 * 1024,
			CacheHitRate:        80.0,
			ConnectionPoolUsage: 85.0,
			GoroutineCount:      800,
		},
		optimizedMetrics: &OptimizedMetrics{
			AvgResponseTime:     200 * time.Millisecond,
			Throughput:          1000,
			ErrorRate:           0.8,
			MemoryUsage:         350 * 1024 * 1024,
			CacheHitRate:        93.0,
			ConnectionPoolUsage: 65.0,
			GoroutineCount:      600,
		},
	}
	
	optimizer.calculateImprovements()
	report := optimizer.GenerateOptimizationReport()
	
	t.Logf("=== Final Optimization Report ===")
	t.Logf("üìä Performance Baseline:")
	t.Logf("  Response Time: %v", report.BaselineMetrics.AvgResponseTime)
	t.Logf("  Throughput: %.1f RPS", report.BaselineMetrics.Throughput)
	t.Logf("  Error Rate: %.2f%%", report.BaselineMetrics.ErrorRate)
	t.Logf("  Memory: %d MB", report.BaselineMetrics.MemoryUsage/(1024*1024))
	
	t.Logf("üìà Performance After Optimization:")
	t.Logf("  Response Time: %v (%.1f%% improvement)", 
		report.OptimizedMetrics.AvgResponseTime, 
		report.OptimizedMetrics.ResponseTimeImprovement)
	t.Logf("  Throughput: %.1f RPS (%.1f%% improvement)", 
		report.OptimizedMetrics.Throughput, 
		report.OptimizedMetrics.ThroughputImprovement)
	t.Logf("  Error Rate: %.2f%%", report.OptimizedMetrics.ErrorRate)
	t.Logf("  Memory: %d MB (%.1f%% efficiency)", 
		report.OptimizedMetrics.MemoryUsage/(1024*1024),
		report.OptimizedMetrics.MemoryEfficiency)
	
	t.Logf("üéØ Optimization Summary:")
	t.Logf("  Issues Identified: %d", report.Summary.TotalIssuesFound)
	t.Logf("  Critical Issues: %d", report.Summary.CriticalIssues)
	t.Logf("  Strategies Applied: %d", report.Summary.StrategiesApplied)
	t.Logf("  Overall Improvement: %.1f%%", report.Summary.OverallImprovement)
	t.Logf("  Business Value: %s", report.Summary.EstimatedBusinessValue)
	
	if len(report.Summary.RecommendedNextSteps) > 0 {
		t.Logf("üîÆ Recommended Next Steps:")
		for i, step := range report.Summary.RecommendedNextSteps {
			t.Logf("  %d. %s", i+1, step)
		}
	}
	
	// ÏÑ±Í≥º ÌèâÍ∞Ä
	if report.Summary.OverallImprovement > 40 {
		t.Logf("üåü Outstanding optimization results!")
	} else if report.Summary.OverallImprovement > 25 {
		t.Logf("‚ú® Excellent optimization results!")
	} else if report.Summary.OverallImprovement > 15 {
		t.Logf("üëç Good optimization results!")
	} else {
		t.Logf("üëå Moderate optimization results")
	}
	
	// Ìà¨Ïûê Í∞ÄÏπò Î∂ÑÏÑù
	if report.Summary.EstimatedBusinessValue == "High" {
		t.Logf("üí∞ High ROI - Strong recommendation for implementation")
	} else if report.Summary.EstimatedBusinessValue == "Medium" {
		t.Logf("üíµ Medium ROI - Consider implementation based on priorities")
	} else {
		t.Logf("üí¥ Low ROI - May not justify immediate implementation cost")
	}
	
	// ÏµúÏ¢Ö Í∂åÍ≥†ÏÇ¨Ìï≠
	t.Logf("=== Final Recommendations ===")
	if report.Summary.CriticalIssues > 0 {
		t.Logf("üö® URGENT: Address %d critical performance issues immediately", 
			report.Summary.CriticalIssues)
	}
	
	if report.Summary.OverallImprovement > 30 {
		t.Logf("‚úÖ RECOMMENDED: Proceed with optimization implementation")
	} else if report.Summary.OverallImprovement > 15 {
		t.Logf("ü§î CONSIDER: Evaluate cost-benefit before implementation")
	} else {
		t.Logf("‚è∏Ô∏è  DEFER: Consider alternative approaches or defer optimization")
	}
	
	// Í≤ÄÏ¶ù
	assert.NotNil(t, report.BaselineMetrics, "Should have baseline metrics")
	assert.NotNil(t, report.OptimizedMetrics, "Should have optimized metrics")
	assert.Greater(t, report.Summary.OverallImprovement, 0.0, "Should show overall improvement")
}