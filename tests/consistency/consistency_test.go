package consistency

import (
	"context"
	"encoding/json"
	"fmt"
	"math/rand"
	"sync"
	"testing"
	"time"

	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"

	"mutual-friend/internal/domain"
	"mutual-friend/pkg/cache"
	"mutual-friend/pkg/config"
	"mutual-friend/pkg/redis"
)

// ConsistencyTester ì¼ê´€ì„± í…ŒìŠ¤íŠ¸ êµ¬ì¡°ì²´
type ConsistencyTester struct {
	cacheService cache.Cache
	ctx          context.Context
	metrics      *ConsistencyMetrics
}

// ConsistencyMetrics ì¼ê´€ì„± ê´€ë ¨ ë©”íŠ¸ë¦­
type ConsistencyMetrics struct {
	mu                        sync.RWMutex
	
	// ë°ì´í„° ì¼ê´€ì„± ë©”íŠ¸ë¦­
	StaleReads               int64
	InconsistentWrites       int64
	MissingUpdates           int64
	DuplicateData            int64
	
	// ìºì‹œ-DB ì¼ê´€ì„±
	CacheDBMismatches        int64
	CacheStaleData           int64
	CacheInvalidationLag     time.Duration
	
	// íŠ¸ëœì­ì…˜ ì¼ê´€ì„±
	IsolationViolations      int64
	AtomicityViolations      int64
	DurabilityViolations     int64
	
	// ì´ë²¤íŠ¸ ì¼ê´€ì„±
	EventOrderViolations     int64
	EventLoss                int64
	DuplicateEvents          int64
	
	// ì‹œê°„ ê¸°ë°˜ ì¼ê´€ì„±
	ClockSkewIssues          int64
	TimestampInconsistencies int64
	CausalOrderViolations    int64
	
	// ë³µêµ¬ ë©”íŠ¸ë¦­
	AutoRecoveryAttempts     int64
	ManualRecoveryRequired   int64
	DataLossIncidents        int64
}

func (cm *ConsistencyMetrics) IncrementStaleRead() {
	cm.mu.Lock()
	defer cm.mu.Unlock()
	cm.StaleReads++
}

func (cm *ConsistencyMetrics) IncrementInconsistentWrite() {
	cm.mu.Lock()
	defer cm.mu.Unlock()
	cm.InconsistentWrites++
}

func (cm *ConsistencyMetrics) IncrementCacheDBMismatch() {
	cm.mu.Lock()
	defer cm.mu.Unlock()
	cm.CacheDBMismatches++
}

func (cm *ConsistencyMetrics) IncrementEventOrderViolation() {
	cm.mu.Lock()
	defer cm.mu.Unlock()
	cm.EventOrderViolations++
}

func (cm *ConsistencyMetrics) AddCacheInvalidationLag(lag time.Duration) {
	cm.mu.Lock()
	defer cm.mu.Unlock()
	cm.CacheInvalidationLag += lag
}

// ConsistencySnapshot ì¼ê´€ì„± ê²€ì¦ì„ ìœ„í•œ ìŠ¤ëƒ…ìƒ·
type ConsistencySnapshot struct {
	Timestamp   time.Time
	CacheState  map[string]interface{}
	DBState     map[string]interface{}
	EventQueue  []Event
	Operations  []Operation
}

// Event ì´ë²¤íŠ¸ êµ¬ì¡°ì²´
type Event struct {
	ID        string
	Type      string
	UserID    string
	FriendID  string
	Timestamp time.Time
	Sequence  int64
}

// Operation ì‘ì—… êµ¬ì¡°ì²´
type Operation struct {
	ID        string
	Type      string
	Target    string
	Before    interface{}
	After     interface{}
	Timestamp time.Time
	Success   bool
}

// NewConsistencyTester ì¼ê´€ì„± í…ŒìŠ¤í„° ìƒì„±
func NewConsistencyTester(t *testing.T) *ConsistencyTester {
	// ì„¤ì • ë¡œë“œ
	cfg, err := config.LoadConfig("../../configs/config.yaml")
	require.NoError(t, err)
	
	// Redis í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
	redisClient, err := redis.NewClient(&redis.Config{
		Address:      cfg.Redis.Address,
		Password:     cfg.Redis.Password,
		Database:     cfg.Redis.Database,
		PoolSize:     cfg.Redis.PoolSize,
		MinIdleConns: cfg.Redis.MinIdleConns,
		MaxRetries:   cfg.Redis.MaxRetries,
		DialTimeout:  time.Duration(cfg.Redis.DialTimeout) * time.Second,
		ReadTimeout:  time.Duration(cfg.Redis.ReadTimeout) * time.Second,
		WriteTimeout: time.Duration(cfg.Redis.WriteTimeout) * time.Second,
	})
	require.NoError(t, err)
	
	// ìºì‹œ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
	cacheService, err := cache.NewService(redisClient)
	require.NoError(t, err)
	
	return &ConsistencyTester{
		cacheService: cacheService,
		ctx:          context.Background(),
		metrics:      &ConsistencyMetrics{},
	}
}

// TestDataConsistency ë°ì´í„° ì¼ê´€ì„± í…ŒìŠ¤íŠ¸
func TestDataConsistency(t *testing.T) {
	tester := NewConsistencyTester(t)
	defer tester.cacheService.Close()
	
	t.Run("Cache-Database Consistency", func(t *testing.T) {
		tester.testCacheDatabaseConsistency(t)
	})
	
	t.Run("Read-Write Consistency", func(t *testing.T) {
		tester.testReadWriteConsistency(t)
	})
	
	t.Run("Eventual Consistency", func(t *testing.T) {
		tester.testEventualConsistency(t)
	})
	
	t.Run("Strong Consistency", func(t *testing.T) {
		tester.testStrongConsistency(t)
	})
}

// testCacheDatabaseConsistency ìºì‹œ-ë°ì´í„°ë² ì´ìŠ¤ ì¼ê´€ì„± í…ŒìŠ¤íŠ¸
func (ct *ConsistencyTester) testCacheDatabaseConsistency(t *testing.T) {
	t.Log("ğŸ” Testing Cache-Database Consistency")
	
	const (
		numOperations = 100
		numCheckers   = 5
		testPrefix    = "consistency_test"
	)
	
	var wg sync.WaitGroup
	var operations []Operation
	var mu sync.Mutex
	
	// ì¼ê´€ì„± ê²€ì¦ì„ ìœ„í•œ ì‹œë®¬ë ˆì´ì…˜ëœ DB ìƒíƒœ
	simulatedDB := make(map[string]interface{})
	var dbMu sync.RWMutex
	
	start := time.Now()
	
	// ë°ì´í„° ì¡°ì‘ ì‘ì—…ë“¤
	for i := 0; i < numOperations; i++ {
		wg.Add(1)
		go func(opID int) {
			defer wg.Done()
			
			key := fmt.Sprintf("%s_%d", testPrefix, opID%10) // í‚¤ ì¶©ëŒ ìœ ë„
			value := fmt.Sprintf("value_%d_%d", opID, time.Now().UnixNano())
			
			operation := Operation{
				ID:        fmt.Sprintf("op_%d", opID),
				Type:      "WRITE",
				Target:    key,
				Timestamp: time.Now(),
			}
			
			// ìºì‹œì— ì“°ê¸°
			cacheStart := time.Now()
			err := ct.cacheService.Set(ct.ctx, key, value, time.Hour)
			cacheTime := time.Since(cacheStart)
			
			if err != nil {
				operation.Success = false
			} else {
				operation.Success = true
				operation.After = value
				
				// ì‹œë®¬ë ˆì´ì…˜ëœ DB ì—…ë°ì´íŠ¸ (ì§€ì—° í¬í•¨)
				time.Sleep(time.Millisecond * time.Duration(rand.Intn(5)+1))
				
				dbMu.Lock()
				simulatedDB[key] = value
				dbMu.Unlock()
			}
			
			mu.Lock()
			operations = append(operations, operation)
			mu.Unlock()
			
			// ìºì‹œ ì§€ì—°ì´ ìˆìœ¼ë©´ ë¹„ì¼ê´€ì„± ê°€ëŠ¥ì„±
			if cacheTime > 50*time.Millisecond {
				ct.metrics.AddCacheInvalidationLag(cacheTime)
			}
		}(i)
	}
	
	// ì¼ê´€ì„± ê²€ì¦ìë“¤
	for c := 0; c < numCheckers; c++ {
		wg.Add(1)
		go func(checkerID int) {
			defer wg.Done()
			
			for i := 0; i < 20; i++ {
				key := fmt.Sprintf("%s_%d", testPrefix, i%10)
				
				// ìºì‹œì—ì„œ ì½ê¸°
				cacheValue, cacheErr := ct.cacheService.Get(ct.ctx, key)
				
				// ì‹œë®¬ë ˆì´ì…˜ëœ DBì—ì„œ ì½ê¸°
				dbMu.RLock()
				dbValue, dbExists := simulatedDB[key]
				dbMu.RUnlock()
				
				// ì¼ê´€ì„± ê²€ì¦
				if cacheErr == nil && dbExists {
					if cacheValue != dbValue {
						ct.metrics.IncrementCacheDBMismatch()
						t.Logf("Mismatch detected - Key: %s, Cache: %s, DB: %v", 
							key, cacheValue, dbValue)
					}
				} else if cacheErr == nil && !dbExists {
					ct.metrics.IncrementStaleRead()
					t.Logf("Stale read detected - Key: %s exists in cache but not in DB", key)
				} else if cacheErr != nil && dbExists {
					ct.metrics.IncrementCacheDBMismatch()
					t.Logf("Cache miss but DB has data - Key: %s", key)
				}
				
				time.Sleep(time.Millisecond * 10)
			}
		}(c)
	}
	
	wg.Wait()
	duration := time.Since(start)
	
	t.Logf("=== Cache-Database Consistency Results ===")
	t.Logf("Total Operations: %d", len(operations))
	t.Logf("Cache-DB Mismatches: %d", ct.metrics.CacheDBMismatches)
	t.Logf("Stale Reads: %d", ct.metrics.StaleReads)
	t.Logf("Average Cache Invalidation Lag: %v", ct.metrics.CacheInvalidationLag/time.Duration(numOperations))
	t.Logf("Duration: %v", duration)
	
	// ì¼ê´€ì„± ê²€ì¦
	successfulOps := 0
	for _, op := range operations {
		if op.Success {
			successfulOps++
		}
	}
	
	consistencyRate := float64(successfulOps-int(ct.metrics.CacheDBMismatches)) / float64(successfulOps) * 100
	t.Logf("Consistency Rate: %.2f%%", consistencyRate)
	
	// ì¼ê´€ì„± ì„ê³„ê°’ ê²€ì¦
	assert.Greater(t, consistencyRate, 90.0, "Consistency rate too low")
	assert.Less(t, ct.metrics.CacheDBMismatches, int64(numOperations/10), "Too many cache-DB mismatches")
}

// testReadWriteConsistency ì½ê¸°-ì“°ê¸° ì¼ê´€ì„± í…ŒìŠ¤íŠ¸
func (ct *ConsistencyTester) testReadWriteConsistency(t *testing.T) {
	t.Log("ğŸ” Testing Read-Write Consistency")
	
	const (
		numWriters = 10
		numReaders = 20
		testKey    = "rw_consistency_test"
	)
	
	var wg sync.WaitGroup
	var writes []string
	var reads []string
	var mu sync.Mutex
	
	// ì´ˆê¸°ê°’ ì„¤ì •
	initialValue := "initial_rw_value"
	err := ct.cacheService.Set(ct.ctx, testKey, initialValue, time.Hour)
	require.NoError(t, err)
	
	start := time.Now()
	
	// ì“°ê¸° ì‘ì—…ë“¤
	for w := 0; w < numWriters; w++ {
		wg.Add(1)
		go func(writerID int) {
			defer wg.Done()
			
			for i := 0; i < 5; i++ {
				value := fmt.Sprintf("writer_%d_value_%d", writerID, i)
				writeTime := time.Now()
				
				err := ct.cacheService.Set(ct.ctx, testKey, value, time.Hour)
				if err == nil {
					mu.Lock()
					writes = append(writes, fmt.Sprintf("%s@%d", value, writeTime.UnixNano()))
					mu.Unlock()
				}
				
				time.Sleep(time.Millisecond * 5)
			}
		}(w)
	}
	
	// ì½ê¸° ì‘ì—…ë“¤
	for r := 0; r < numReaders; r++ {
		wg.Add(1)
		go func(readerID int) {
			defer wg.Done()
			
			for i := 0; i < 10; i++ {
				readTime := time.Now()
				value, err := ct.cacheService.Get(ct.ctx, testKey)
				
				if err == nil {
					mu.Lock()
					reads = append(reads, fmt.Sprintf("%s@%d", value, readTime.UnixNano()))
					mu.Unlock()
				}
				
				time.Sleep(time.Millisecond * 3)
			}
		}(r)
	}
	
	wg.Wait()
	duration := time.Since(start)
	
	// ì½ê¸°-ì“°ê¸° ì¼ê´€ì„± ë¶„ì„
	inconsistencies := ct.analyzeReadWriteConsistency(writes, reads, initialValue)
	
	t.Logf("=== Read-Write Consistency Results ===")
	t.Logf("Total Writes: %d", len(writes))
	t.Logf("Total Reads: %d", len(reads))
	t.Logf("Inconsistencies Detected: %d", inconsistencies)
	t.Logf("Duration: %v", duration)
	
	if inconsistencies > 0 {
		t.Logf("ğŸ”´ Read-write consistency violations: %d", inconsistencies)
	}
	
	// ì¼ê´€ì„± ê²€ì¦
	assert.Less(t, inconsistencies, len(reads)/10, "Too many read-write inconsistencies")
}

// analyzeReadWriteConsistency ì½ê¸°-ì“°ê¸° ì¼ê´€ì„± ë¶„ì„
func (ct *ConsistencyTester) analyzeReadWriteConsistency(writes, reads []string, initialValue string) int {
	inconsistencies := 0
	
	// íƒ€ì„ìŠ¤íƒ¬í”„ë³„ë¡œ ì“°ê¸° ì‘ì—… ì •ë ¬
	writeMap := make(map[int64]string)
	for _, write := range writes {
		parts := parseTimestampedValue(write)
		if len(parts) == 2 {
			writeMap[parts[1]] = parts[0]
		}
	}
	
	// ê° ì½ê¸°ì— ëŒ€í•´ ì¼ê´€ì„± ê²€ì¦
	for _, read := range reads {
		parts := parseTimestampedValue(read)
		if len(parts) != 2 {
			continue
		}
		
		readValue, readTime := parts[0], parts[1]
		
		// ì½ê¸° ì‹œì  ì´í›„ì— ì“°ì¸ ê°’ì„ ì½ì—ˆë‹¤ë©´ ì¼ê´€ì„± ìœ„ë°˜
		validValue := ct.findValidValueAtTime(writeMap, readTime, initialValue)
		
		if readValue != validValue {
			inconsistencies++
			ct.metrics.IncrementInconsistentWrite()
		}
	}
	
	return inconsistencies
}

// parseTimestampedValue íƒ€ì„ìŠ¤íƒ¬í”„ê°€ í¬í•¨ëœ ê°’ íŒŒì‹±
func parseTimestampedValue(timestampedValue string) []interface{} {
	var value string
	var timestamp int64
	
	n, err := fmt.Sscanf(timestampedValue, "%s@%d", &value, &timestamp)
	if n == 2 && err == nil {
		return []interface{}{value, timestamp}
	}
	
	return nil
}

// findValidValueAtTime íŠ¹ì • ì‹œì ì—ì„œ ìœ íš¨í•œ ê°’ ì°¾ê¸°
func (ct *ConsistencyTester) findValidValueAtTime(writeMap map[int64]string, readTime int64, initialValue string) string {
	validValue := initialValue
	validTime := int64(0)
	
	for writeTime, writeValue := range writeMap {
		if writeTime <= readTime && writeTime > validTime {
			validValue = writeValue
			validTime = writeTime
		}
	}
	
	return validValue
}

// testEventualConsistency ìµœì¢… ì¼ê´€ì„± í…ŒìŠ¤íŠ¸
func (ct *ConsistencyTester) testEventualConsistency(t *testing.T) {
	t.Log("ğŸ” Testing Eventual Consistency")
	
	const (
		numNodes       = 5
		operationsPerNode = 20
		testKey        = "eventual_consistency_test"
	)
	
	var wg sync.WaitGroup
	var allOperations []Operation
	var mu sync.Mutex
	
	// ê° ë…¸ë“œì—ì„œ ë…ë¦½ì ì¸ ì‘ì—… ìˆ˜í–‰
	for node := 0; node < numNodes; node++ {
		wg.Add(1)
		go func(nodeID int) {
			defer wg.Done()
			
			for op := 0; op < operationsPerNode; op++ {
				operation := Operation{
					ID:        fmt.Sprintf("node_%d_op_%d", nodeID, op),
					Type:      "UPDATE",
					Target:    testKey,
					Timestamp: time.Now(),
				}
				
				value := fmt.Sprintf("node_%d_value_%d", nodeID, op)
				err := ct.cacheService.Set(ct.ctx, testKey, value, time.Hour)
				
				operation.Success = err == nil
				operation.After = value
				
				mu.Lock()
				allOperations = append(allOperations, operation)
				mu.Unlock()
				
				// ë…¸ë“œ ê°„ ì§€ì—° ì‹œë®¬ë ˆì´ì…˜
				time.Sleep(time.Millisecond * time.Duration(rand.Intn(10)+1))
			}
		}(node)
	}
	
	wg.Wait()
	
	// ìµœì¢… ì¼ê´€ì„± ìˆ˜ë ´ ëŒ€ê¸°
	convergenceTimeout := 5 * time.Second
	converged := ct.waitForEventualConsistency(testKey, convergenceTimeout)
	
	// ìµœì¢… ê°’ í™•ì¸
	finalValue, err := ct.cacheService.Get(ct.ctx, testKey)
	require.NoError(t, err)
	
	t.Logf("=== Eventual Consistency Results ===")
	t.Logf("Total Operations: %d", len(allOperations))
	t.Logf("Converged: %t", converged)
	t.Logf("Final Value: %s", finalValue)
	
	// ìµœì¢… ê°’ì´ ì‹¤ì œ ì‘ì—… ì¤‘ í•˜ë‚˜ì¸ì§€ í™•ì¸
	isValidFinalValue := false
	for _, op := range allOperations {
		if op.Success && op.After == finalValue {
			isValidFinalValue = true
			break
		}
	}
	
	t.Logf("Final Value is Valid: %t", isValidFinalValue)
	assert.True(t, isValidFinalValue, "Final value is not from any performed operation")
	
	if !converged {
		t.Logf("âš ï¸  System did not converge within timeout")
	}
}

// waitForEventualConsistency ìµœì¢… ì¼ê´€ì„± ìˆ˜ë ´ ëŒ€ê¸°
func (ct *ConsistencyTester) waitForEventualConsistency(key string, timeout time.Duration) bool {
	start := time.Now()
	lastValue := ""
	stableCount := 0
	requiredStableReads := 10
	
	for time.Since(start) < timeout {
		value, err := ct.cacheService.Get(ct.ctx, key)
		if err != nil {
			time.Sleep(100 * time.Millisecond)
			continue
		}
		
		if value == lastValue {
			stableCount++
			if stableCount >= requiredStableReads {
				return true
			}
		} else {
			stableCount = 0
			lastValue = value
		}
		
		time.Sleep(100 * time.Millisecond)
	}
	
	return false
}

// testStrongConsistency ê°•í•œ ì¼ê´€ì„± í…ŒìŠ¤íŠ¸
func (ct *ConsistencyTester) testStrongConsistency(t *testing.T) {
	t.Log("ğŸ” Testing Strong Consistency")
	
	const (
		numClients = 20
		testKey    = "strong_consistency_test"
	)
	
	var wg sync.WaitGroup
	var readResults []string
	var mu sync.Mutex
	
	// ê°•í•œ ì¼ê´€ì„±ì„ ìœ„í•œ ìˆœì°¨ì  ì“°ê¸°
	sequentialValues := make([]string, 10)
	for i := 0; i < 10; i++ {
		value := fmt.Sprintf("sequential_value_%d", i)
		sequentialValues[i] = value
		
		err := ct.cacheService.Set(ct.ctx, testKey, value, time.Hour)
		require.NoError(t, err)
		
		time.Sleep(50 * time.Millisecond) // ê°•í•œ ì¼ê´€ì„±ì„ ìœ„í•œ ëŒ€ê¸°
	}
	
	// ë™ì‹œ ì½ê¸°ë¡œ ê°•í•œ ì¼ê´€ì„± ê²€ì¦
	finalValue := sequentialValues[len(sequentialValues)-1]
	
	for c := 0; c < numClients; c++ {
		wg.Add(1)
		go func(clientID int) {
			defer wg.Done()
			
			value, err := ct.cacheService.Get(ct.ctx, testKey)
			if err == nil {
				mu.Lock()
				readResults = append(readResults, value)
				mu.Unlock()
			}
		}(c)
	}
	
	wg.Wait()
	
	// ê°•í•œ ì¼ê´€ì„± ê²€ì¦
	consistentReads := 0
	for _, readValue := range readResults {
		if readValue == finalValue {
			consistentReads++
		}
	}
	
	t.Logf("=== Strong Consistency Results ===")
	t.Logf("Total Reads: %d", len(readResults))
	t.Logf("Consistent Reads: %d", consistentReads)
	t.Logf("Consistency Rate: %.2f%%", float64(consistentReads)/float64(len(readResults))*100)
	
	// ê°•í•œ ì¼ê´€ì„±ì€ 100% ì¼ê´€ì„±ì„ ìš”êµ¬
	assert.Equal(t, len(readResults), consistentReads, "Strong consistency violation")
}

// TestEventConsistency ì´ë²¤íŠ¸ ì¼ê´€ì„± í…ŒìŠ¤íŠ¸
func TestEventConsistency(t *testing.T) {
	tester := NewConsistencyTester(t)
	defer tester.cacheService.Close()
	
	t.Run("Event Ordering", func(t *testing.T) {
		tester.testEventOrdering(t)
	})
	
	t.Run("Event Deduplication", func(t *testing.T) {
		tester.testEventDeduplication(t)
	})
	
	t.Run("Event Replay Consistency", func(t *testing.T) {
		tester.testEventReplayConsistency(t)
	})
}

// testEventOrdering ì´ë²¤íŠ¸ ìˆœì„œ í…ŒìŠ¤íŠ¸
func (ct *ConsistencyTester) testEventOrdering(t *testing.T) {
	t.Log("ğŸ” Testing Event Ordering Consistency")
	
	const (
		numEvents = 100
		testKey   = "event_ordering_test"
	)
	
	var events []Event
	var processedEvents []Event
	var mu sync.Mutex
	
	// ìˆœì°¨ì  ì´ë²¤íŠ¸ ìƒì„±
	for i := 0; i < numEvents; i++ {
		event := Event{
			ID:        fmt.Sprintf("event_%03d", i),
			Type:      "UPDATE",
			UserID:    "test_user",
			FriendID:  fmt.Sprintf("friend_%d", i%10),
			Timestamp: time.Now().Add(time.Duration(i) * time.Millisecond),
			Sequence:  int64(i),
		}
		events = append(events, event)
	}
	
	// ì´ë²¤íŠ¸ë¥¼ ëœë¤ ìˆœì„œë¡œ ì²˜ë¦¬ (ì‹¤ì œ ë¶„ì‚° ì‹œìŠ¤í…œì—ì„œ ë°œìƒ ê°€ëŠ¥)
	shuffledEvents := make([]Event, len(events))
	copy(shuffledEvents, events)
	rand.Shuffle(len(shuffledEvents), func(i, j int) {
		shuffledEvents[i], shuffledEvents[j] = shuffledEvents[j], shuffledEvents[i]
	})
	
	var wg sync.WaitGroup
	
	// ì´ë²¤íŠ¸ ë³‘ë ¬ ì²˜ë¦¬
	for _, event := range shuffledEvents {
		wg.Add(1)
		go func(e Event) {
			defer wg.Done()
			
			// ì´ë²¤íŠ¸ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
			key := fmt.Sprintf("%s_%s", testKey, e.UserID)
			value := fmt.Sprintf("event_%s_processed", e.ID)
			
			err := ct.cacheService.Set(ct.ctx, key, value, time.Hour)
			if err == nil {
				mu.Lock()
				processedEvents = append(processedEvents, e)
				mu.Unlock()
			}
			
			// ì²˜ë¦¬ ì§€ì—° ì‹œë®¬ë ˆì´ì…˜
			time.Sleep(time.Millisecond * time.Duration(rand.Intn(5)+1))
		}(event)
	}
	
	wg.Wait()
	
	// ì´ë²¤íŠ¸ ìˆœì„œ ê²€ì¦
	orderViolations := ct.analyzeEventOrdering(events, processedEvents)
	
	t.Logf("=== Event Ordering Results ===")
	t.Logf("Total Events: %d", len(events))
	t.Logf("Processed Events: %d", len(processedEvents))
	t.Logf("Order Violations: %d", orderViolations)
	
	if orderViolations > 0 {
		t.Logf("ğŸ”´ Event ordering violations detected: %d", orderViolations)
		ct.metrics.EventOrderViolations += int64(orderViolations)
	}
	
	// ìˆœì„œ ìœ„ë°˜ì´ ì „ì²´ì˜ 10% ì´í•˜ì—¬ì•¼ í•¨
	assert.Less(t, orderViolations, len(events)/10, "Too many event ordering violations")
}

// analyzeEventOrdering ì´ë²¤íŠ¸ ìˆœì„œ ë¶„ì„
func (ct *ConsistencyTester) analyzeEventOrdering(originalEvents, processedEvents []Event) int {
	// ì²˜ë¦¬ëœ ì´ë²¤íŠ¸ë¥¼ ì‹œí€€ìŠ¤ ë²ˆí˜¸ë¡œ ì •ë ¬
	eventsBySequence := make(map[int64]Event)
	for _, event := range processedEvents {
		eventsBySequence[event.Sequence] = event
	}
	
	violations := 0
	lastSequence := int64(-1)
	
	// ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆëŠ”ì§€ í™•ì¸
	for i := 0; i < len(originalEvents); i++ {
		expectedSeq := int64(i)
		if event, exists := eventsBySequence[expectedSeq]; exists {
			if event.Sequence < lastSequence {
				violations++
			}
			lastSequence = event.Sequence
		}
	}
	
	return violations
}

// testEventDeduplication ì´ë²¤íŠ¸ ì¤‘ë³µ ì œê±° í…ŒìŠ¤íŠ¸
func (ct *ConsistencyTester) testEventDeduplication(t *testing.T) {
	t.Log("ğŸ” Testing Event Deduplication")
	
	const (
		uniqueEvents = 50
		duplicateCount = 3
		testKey = "dedup_test"
	)
	
	var allEvents []Event
	var processedValues []string
	var mu sync.Mutex
	
	// ì¤‘ë³µ ì´ë²¤íŠ¸ ìƒì„±
	for i := 0; i < uniqueEvents; i++ {
		baseEvent := Event{
			ID:        fmt.Sprintf("unique_event_%d", i),
			Type:      "ADD_FRIEND",
			UserID:    "test_user",
			FriendID:  fmt.Sprintf("friend_%d", i),
			Timestamp: time.Now(),
			Sequence:  int64(i),
		}
		
		// ì›ë³¸ + ì¤‘ë³µë³¸ë“¤
		for d := 0; d < duplicateCount; d++ {
			duplicateEvent := baseEvent
			duplicateEvent.ID = fmt.Sprintf("%s_dup_%d", baseEvent.ID, d)
			allEvents = append(allEvents, duplicateEvent)
		}
	}
	
	// ì¤‘ë³µ ì´ë²¤íŠ¸ ëœë¤ ì²˜ë¦¬
	rand.Shuffle(len(allEvents), func(i, j int) {
		allEvents[i], allEvents[j] = allEvents[j], allEvents[i]
	})
	
	var wg sync.WaitGroup
	
	for _, event := range allEvents {
		wg.Add(1)
		go func(e Event) {
			defer wg.Done()
			
			// ì¤‘ë³µ ì œê±° ì‹œë®¬ë ˆì´ì…˜ (ê°„ë‹¨í•œ í‚¤ ê¸°ë°˜)
			dedupKey := fmt.Sprintf("%s_dedup_%s_%s", testKey, e.UserID, e.FriendID)
			
			// ì¤‘ë³µ í™•ì¸
			existing, err := ct.cacheService.Get(ct.ctx, dedupKey)
			if err != nil || existing == "" {
				// ìƒˆë¡œìš´ ì´ë²¤íŠ¸ ì²˜ë¦¬
				value := fmt.Sprintf("processed_%s", e.FriendID)
				ct.cacheService.Set(ct.ctx, dedupKey, value, time.Hour)
				
				mu.Lock()
				processedValues = append(processedValues, value)
				mu.Unlock()
			}
			// ì¤‘ë³µ ì´ë²¤íŠ¸ëŠ” ë¬´ì‹œ
		}(event)
	}
	
	wg.Wait()
	
	// ì¤‘ë³µ ì œê±° íš¨ê³¼ ê²€ì¦
	uniqueProcessedValues := make(map[string]bool)
	for _, value := range processedValues {
		uniqueProcessedValues[value] = true
	}
	
	t.Logf("=== Event Deduplication Results ===")
	t.Logf("Total Events: %d", len(allEvents))
	t.Logf("Unique Events: %d", uniqueEvents)
	t.Logf("Processed Values: %d", len(processedValues))
	t.Logf("Unique Processed Values: %d", len(uniqueProcessedValues))
	
	duplicatesProcessed := len(processedValues) - len(uniqueProcessedValues)
	t.Logf("Duplicates Processed: %d", duplicatesProcessed)
	
	// ì¤‘ë³µ ì œê±° íš¨ìœ¨ì„± ê²€ì¦
	assert.LessOrEqual(t, len(uniqueProcessedValues), uniqueEvents, 
		"More unique values processed than unique events")
	
	if duplicatesProcessed > 0 {
		t.Logf("ğŸŸ¡ Duplicate events processed: %d", duplicatesProcessed)
		ct.metrics.DuplicateEvents += int64(duplicatesProcessed)
	}
}

// testEventReplayConsistency ì´ë²¤íŠ¸ ì¬ìƒ ì¼ê´€ì„± í…ŒìŠ¤íŠ¸
func (ct *ConsistencyTester) testEventReplayConsistency(t *testing.T) {
	t.Log("ğŸ” Testing Event Replay Consistency")
	
	const testKey = "replay_test"
	
	// ì›ë³¸ ì´ë²¤íŠ¸ ì‹œí€€ìŠ¤
	originalEvents := []Event{
		{ID: "1", Type: "SET", UserID: "user1", Timestamp: time.Now(), Sequence: 1},
		{ID: "2", Type: "UPDATE", UserID: "user1", Timestamp: time.Now().Add(time.Second), Sequence: 2},
		{ID: "3", Type: "DELETE", UserID: "user1", Timestamp: time.Now().Add(2*time.Second), Sequence: 3},
		{ID: "4", Type: "SET", UserID: "user1", Timestamp: time.Now().Add(3*time.Second), Sequence: 4},
	}
	
	// ì²« ë²ˆì§¸ ì‹¤í–‰: ì •ìƒ ìˆœì„œë¡œ ì´ë²¤íŠ¸ ì²˜ë¦¬
	firstRunState := ct.processEventSequence(originalEvents, "first_run")
	
	// ìºì‹œ ì´ˆê¸°í™”
	ct.cacheService.Flush(ct.ctx)
	
	// ë‘ ë²ˆì§¸ ì‹¤í–‰: ê°™ì€ ì´ë²¤íŠ¸ ì¬ìƒ
	secondRunState := ct.processEventSequence(originalEvents, "second_run")
	
	// ì„¸ ë²ˆì§¸ ì‹¤í–‰: ë¶€ë¶„ ì¬ìƒ (ì¥ì•  ë³µêµ¬ ì‹œë®¬ë ˆì´ì…˜)
	ct.cacheService.Flush(ct.ctx)
	partialEvents := originalEvents[1:] // ì²« ë²ˆì§¸ ì´ë²¤íŠ¸ ëˆ„ë½
	thirdRunState := ct.processEventSequence(partialEvents, "third_run")
	
	t.Logf("=== Event Replay Consistency Results ===")
	t.Logf("First Run Final State: %s", firstRunState)
	t.Logf("Second Run Final State: %s", secondRunState)
	t.Logf("Third Run Final State: %s", thirdRunState)
	
	// ì¬ìƒ ì¼ê´€ì„± ê²€ì¦
	if firstRunState == secondRunState {
		t.Logf("âœ… Replay consistency maintained")
	} else {
		t.Logf("ğŸ”´ Replay consistency violation")
		ct.metrics.IncrementEventOrderViolation()
	}
	
	// ë¶€ë¶„ ì¬ìƒ ê²€ì¦
	if firstRunState != thirdRunState {
		t.Logf("ğŸŸ¡ Partial replay produced different result (expected)")
	}
	
	assert.Equal(t, firstRunState, secondRunState, "Event replay inconsistency")
}

// processEventSequence ì´ë²¤íŠ¸ ì‹œí€€ìŠ¤ ì²˜ë¦¬
func (ct *ConsistencyTester) processEventSequence(events []Event, runID string) string {
	key := fmt.Sprintf("replay_test_%s", runID)
	
	for _, event := range events {
		switch event.Type {
		case "SET":
			value := fmt.Sprintf("value_from_event_%s", event.ID)
			ct.cacheService.Set(ct.ctx, key, value, time.Hour)
		case "UPDATE":
			existing, err := ct.cacheService.Get(ct.ctx, key)
			if err == nil {
				updated := fmt.Sprintf("%s_updated_by_%s", existing, event.ID)
				ct.cacheService.Set(ct.ctx, key, updated, time.Hour)
			}
		case "DELETE":
			ct.cacheService.Delete(ct.ctx, key)
		}
		
		time.Sleep(10 * time.Millisecond) // ì´ë²¤íŠ¸ ê°„ ì§€ì—°
	}
	
	// ìµœì¢… ìƒíƒœ ë°˜í™˜
	finalState, err := ct.cacheService.Get(ct.ctx, key)
	if err != nil {
		return "DELETED"
	}
	return finalState
}

// TestTimestampConsistency íƒ€ì„ìŠ¤íƒ¬í”„ ì¼ê´€ì„± í…ŒìŠ¤íŠ¸
func TestTimestampConsistency(t *testing.T) {
	tester := NewConsistencyTester(t)
	defer tester.cacheService.Close()
	
	t.Run("Clock Skew Detection", func(t *testing.T) {
		tester.testClockSkewDetection(t)
	})
	
	t.Run("Causal Order Consistency", func(t *testing.T) {
		tester.testCausalOrderConsistency(t)
	})
}

// testClockSkewDetection ì‹œê³„ í¸ì°¨ ê°ì§€ í…ŒìŠ¤íŠ¸
func (ct *ConsistencyTester) testClockSkewDetection(t *testing.T) {
	t.Log("ğŸ” Testing Clock Skew Detection")
	
	const (
		numNodes = 5
		testKey  = "clock_skew_test"
	)
	
	var timestamps []time.Time
	var mu sync.Mutex
	
	var wg sync.WaitGroup
	
	// ê° ë…¸ë“œì—ì„œ ì‹œë®¬ë ˆì´ì…˜ëœ ì‹œê³„ í¸ì°¨ë¡œ ì‘ì—…
	for node := 0; node < numNodes; node++ {
		wg.Add(1)
		go func(nodeID int) {
			defer wg.Done()
			
			// ì‹œê³„ í¸ì°¨ ì‹œë®¬ë ˆì´ì…˜ (-100ms ~ +100ms)
			skew := time.Duration((nodeID-2)*50) * time.Millisecond
			nodeTime := time.Now().Add(skew)
			
			value := fmt.Sprintf("node_%d_time_%d", nodeID, nodeTime.UnixNano())
			err := ct.cacheService.Set(ct.ctx, testKey, value, time.Hour)
			
			if err == nil {
				mu.Lock()
				timestamps = append(timestamps, nodeTime)
				mu.Unlock()
			}
		}(node)
	}
	
	wg.Wait()
	
	// ì‹œê³„ í¸ì°¨ ë¶„ì„
	if len(timestamps) > 1 {
		var minTime, maxTime time.Time
		minTime = timestamps[0]
		maxTime = timestamps[0]
		
		for _, ts := range timestamps {
			if ts.Before(minTime) {
				minTime = ts
			}
			if ts.After(maxTime) {
				maxTime = ts
			}
		}
		
		skewRange := maxTime.Sub(minTime)
		
		t.Logf("=== Clock Skew Detection Results ===")
		t.Logf("Timestamps Collected: %d", len(timestamps))
		t.Logf("Skew Range: %v", skewRange)
		
		if skewRange > 200*time.Millisecond {
			t.Logf("ğŸ”´ Significant clock skew detected: %v", skewRange)
			ct.metrics.ClockSkewIssues++
		}
		
		assert.Less(t, skewRange, 500*time.Millisecond, "Clock skew too large")
	}
}

// testCausalOrderConsistency ì¸ê³¼ ìˆœì„œ ì¼ê´€ì„± í…ŒìŠ¤íŠ¸
func (ct *ConsistencyTester) testCausalOrderConsistency(t *testing.T) {
	t.Log("ğŸ” Testing Causal Order Consistency")
	
	const testKey = "causal_order_test"
	
	// ì¸ê³¼ê´€ê³„ ì²´ì¸: A â†’ B â†’ C
	var wg sync.WaitGroup
	
	// ì´ë²¤íŠ¸ A
	wg.Add(1)
	go func() {
		defer wg.Done()
		
		err := ct.cacheService.Set(ct.ctx, testKey, "event_A", time.Hour)
		if err != nil {
			t.Logf("Event A failed: %v", err)
		}
	}()
	
	wg.Wait()
	time.Sleep(50 * time.Millisecond) // ì¸ê³¼ê´€ê³„ ë³´ì¥
	
	// ì´ë²¤íŠ¸ B (Aì— ì˜ì¡´)
	wg.Add(1)
	go func() {
		defer wg.Done()
		
		value, err := ct.cacheService.Get(ct.ctx, testKey)
		if err == nil && value == "event_A" {
			ct.cacheService.Set(ct.ctx, testKey, "event_B_after_A", time.Hour)
		} else {
			ct.metrics.CausalOrderViolations++
			t.Logf("Causal order violation: B executed without A")
		}
	}()
	
	wg.Wait()
	time.Sleep(50 * time.Millisecond)
	
	// ì´ë²¤íŠ¸ C (Bì— ì˜ì¡´)
	wg.Add(1)
	go func() {
		defer wg.Done()
		
		value, err := ct.cacheService.Get(ct.ctx, testKey)
		if err == nil && value == "event_B_after_A" {
			ct.cacheService.Set(ct.ctx, testKey, "event_C_after_B", time.Hour)
		} else {
			ct.metrics.CausalOrderViolations++
			t.Logf("Causal order violation: C executed without B")
		}
	}()
	
	wg.Wait()
	
	// ìµœì¢… ì¸ê³¼ê´€ê³„ ê²€ì¦
	finalValue, err := ct.cacheService.Get(ct.ctx, testKey)
	require.NoError(t, err)
	
	t.Logf("=== Causal Order Consistency Results ===")
	t.Logf("Final Value: %s", finalValue)
	t.Logf("Causal Order Violations: %d", ct.metrics.CausalOrderViolations)
	
	expectedFinalValue := "event_C_after_B"
	if finalValue == expectedFinalValue {
		t.Logf("âœ… Causal order maintained")
	} else {
		t.Logf("ğŸ”´ Causal order violation detected")
	}
	
	assert.Equal(t, expectedFinalValue, finalValue, "Causal order consistency violation")
}

// TestConsistencySummary ì¼ê´€ì„± í…ŒìŠ¤íŠ¸ ì¢…í•© ë³´ê³ ì„œ
func TestConsistencySummary(t *testing.T) {
	tester := NewConsistencyTester(t)
	defer tester.cacheService.Close()
	
	t.Log("ğŸ“Š Consistency Test Summary Report")
	
	// ì „ì²´ ë©”íŠ¸ë¦­ ìš”ì•½
	t.Logf("=== Data Consistency Issues ===")
	t.Logf("Stale Reads: %d", tester.metrics.StaleReads)
	t.Logf("Inconsistent Writes: %d", tester.metrics.InconsistentWrites)
	t.Logf("Missing Updates: %d", tester.metrics.MissingUpdates)
	t.Logf("Cache-DB Mismatches: %d", tester.metrics.CacheDBMismatches)
	
	t.Logf("=== Transaction Consistency ===")
	t.Logf("Isolation Violations: %d", tester.metrics.IsolationViolations)
	t.Logf("Atomicity Violations: %d", tester.metrics.AtomicityViolations)
	t.Logf("Durability Violations: %d", tester.metrics.DurabilityViolations)
	
	t.Logf("=== Event Consistency ===")
	t.Logf("Event Order Violations: %d", tester.metrics.EventOrderViolations)
	t.Logf("Event Loss: %d", tester.metrics.EventLoss)
	t.Logf("Duplicate Events: %d", tester.metrics.DuplicateEvents)
	
	t.Logf("=== Temporal Consistency ===")
	t.Logf("Clock Skew Issues: %d", tester.metrics.ClockSkewIssues)
	t.Logf("Timestamp Inconsistencies: %d", tester.metrics.TimestampInconsistencies)
	t.Logf("Causal Order Violations: %d", tester.metrics.CausalOrderViolations)
	
	// ì‹¬ê°ë„ ë¶„ì„
	criticalIssues := tester.metrics.AtomicityViolations + tester.metrics.DurabilityViolations + tester.metrics.DataLossIncidents
	majorIssues := tester.metrics.IsolationViolations + tester.metrics.EventOrderViolations + tester.metrics.CausalOrderViolations
	minorIssues := tester.metrics.StaleReads + tester.metrics.CacheDBMismatches + tester.metrics.DuplicateEvents
	
	t.Logf("=== Consistency Issue Severity ===")
	t.Logf("Critical Issues: %d", criticalIssues)
	t.Logf("Major Issues: %d", majorIssues)
	t.Logf("Minor Issues: %d", minorIssues)
	
	// ê°œì„  ê¶Œê³ ì‚¬í•­
	t.Logf("=== Consistency Improvement Recommendations ===")
	if criticalIssues > 0 {
		t.Logf("ğŸ”´ CRITICAL: Implement stronger consistency guarantees")
		t.Logf("ğŸ”´ CRITICAL: Add transaction support and rollback mechanisms")
	}
	if majorIssues > 0 {
		t.Logf("ğŸŸ¡ MAJOR: Implement proper isolation levels")
		t.Logf("ğŸŸ¡ MAJOR: Add event ordering and causality tracking")
	}
	if minorIssues > 0 {
		t.Logf("ğŸŸ¢ MINOR: Optimize cache invalidation patterns")
		t.Logf("ğŸŸ¢ MINOR: Implement better duplicate detection")
	}
	
	// ì „ì²´ ì¼ê´€ì„± ì ìˆ˜
	totalIssues := criticalIssues + majorIssues + minorIssues
	consistencyScore := 100.0
	
	if totalIssues > 0 {
		// ê°€ì¤‘ì¹˜ ì ìš©í•˜ì—¬ ì ìˆ˜ ê³„ì‚°
		weightedScore := float64(criticalIssues)*10 + float64(majorIssues)*3 + float64(minorIssues)*1
		consistencyScore = math.Max(0, 100.0-weightedScore)
	}
	
	t.Logf("=== Overall Consistency Score ===")
	t.Logf("Consistency Score: %.1f/100", consistencyScore)
	
	if consistencyScore >= 90 {
		t.Logf("âœ… Excellent consistency")
	} else if consistencyScore >= 70 {
		t.Logf("ğŸŸ¡ Good consistency with room for improvement")
	} else {
		t.Logf("ğŸ”´ Poor consistency - immediate attention required")
	}
}